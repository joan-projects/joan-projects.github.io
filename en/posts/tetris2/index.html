<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning in Tetris – Part 2 | Joan Projects</title><meta name=keywords content><meta name=description content="Introduction
In this second installment I’m going to explain how I tackled the challenge of training a Reinforcement Learning agent for Tetris, taking into account the difficulties I mentioned in the previous article.
The main goal is to reach a superhuman level—not in the sense that the agent plays faster or with greater precision than a person (that, after all, is expected from a machine), but because it becomes a better strategist. In fact, our agent will play at a speed similar to that of a strong human (fewer than 2.5 pieces per second), and I’ll even force it to make mistakes from time to time (5% probability of making an error).
I’ll tell you about the first approaches that yielded good results and served as the foundation for the ones that followed, which I’ll omit here to avoid making the article too long or overly technical or mathematical.
I imagine the first thing you did was hit play and notice the surprising level achieved. It’s remarkable and quite a milestone for reinforcement learning. I managed to get an AI to learn effective strategies by itself, without receiving any advice; as you can see, it plans, can anticipate future situations without knowing the next pieces, performs combos, and doesn’t chase immediate reward.
So if the video already piqued your curiosity, I invite you to keep reading. Let’s go!"><meta name=author content="Joan"><link rel=canonical href=https://joan-projects.github.io/en/posts/tetris2/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://joan-projects.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://joan-projects.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://joan-projects.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://joan-projects.github.io/apple-touch-icon.png><link rel=mask-icon href=https://joan-projects.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=es href=https://joan-projects.github.io/posts/tetris2/><link rel=alternate hreflang=en href=https://joan-projects.github.io/en/posts/tetris2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js crossorigin=anonymous></script><style>.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><style>.vc-entry{position:relative}.vc-poster{position:relative;display:block;width:100%;padding:0;border:0;background:0 0;cursor:pointer}.vc-poster .entry-cover-image{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-entry video{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-play{position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);font-size:3rem;line-height:1;opacity:.9;background:rgba(0,0,0,.35);color:#fff;padding:.25em .5em;border-radius:.5em}@media(max-width:640px){.vc-play{font-size:2.2rem}}.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><meta property="og:url" content="https://joan-projects.github.io/en/posts/tetris2/"><meta property="og:site_name" content="Joan Projects"><meta property="og:title" content="Reinforcement Learning in Tetris – Part 2"><meta property="og:description" content="Introduction In this second installment I’m going to explain how I tackled the challenge of training a Reinforcement Learning agent for Tetris, taking into account the difficulties I mentioned in the previous article.
The main goal is to reach a superhuman level—not in the sense that the agent plays faster or with greater precision than a person (that, after all, is expected from a machine), but because it becomes a better strategist. In fact, our agent will play at a speed similar to that of a strong human (fewer than 2.5 pieces per second), and I’ll even force it to make mistakes from time to time (5% probability of making an error).
I’ll tell you about the first approaches that yielded good results and served as the foundation for the ones that followed, which I’ll omit here to avoid making the article too long or overly technical or mathematical.
I imagine the first thing you did was hit play and notice the surprising level achieved. It’s remarkable and quite a milestone for reinforcement learning. I managed to get an AI to learn effective strategies by itself, without receiving any advice; as you can see, it plans, can anticipate future situations without knowing the next pieces, performs combos, and doesn’t chase immediate reward.
So if the video already piqued your curiosity, I invite you to keep reading. Let’s go!"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-05T00:00:00+00:00"><meta property="og:image" content="https://joan-projects.github.io/img/Tetris2.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://joan-projects.github.io/img/Tetris2.png"><meta name=twitter:title content="Reinforcement Learning in Tetris – Part 2"><meta name=twitter:description content="Introduction
In this second installment I’m going to explain how I tackled the challenge of training a Reinforcement Learning agent for Tetris, taking into account the difficulties I mentioned in the previous article.
The main goal is to reach a superhuman level—not in the sense that the agent plays faster or with greater precision than a person (that, after all, is expected from a machine), but because it becomes a better strategist. In fact, our agent will play at a speed similar to that of a strong human (fewer than 2.5 pieces per second), and I’ll even force it to make mistakes from time to time (5% probability of making an error).
I’ll tell you about the first approaches that yielded good results and served as the foundation for the ones that followed, which I’ll omit here to avoid making the article too long or overly technical or mathematical.
I imagine the first thing you did was hit play and notice the surprising level achieved. It’s remarkable and quite a milestone for reinforcement learning. I managed to get an AI to learn effective strategies by itself, without receiving any advice; as you can see, it plans, can anticipate future situations without knowing the next pieces, performs combos, and doesn’t chase immediate reward.
So if the video already piqued your curiosity, I invite you to keep reading. Let’s go!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://joan-projects.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning in Tetris – Part 2","item":"https://joan-projects.github.io/en/posts/tetris2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning in Tetris – Part 2","name":"Reinforcement Learning in Tetris – Part 2","description":"Introduction In this second installment I’m going to explain how I tackled the challenge of training a Reinforcement Learning agent for Tetris, taking into account the difficulties I mentioned in the previous article.\nThe main goal is to reach a superhuman level—not in the sense that the agent plays faster or with greater precision than a person (that, after all, is expected from a machine), but because it becomes a better strategist. In fact, our agent will play at a speed similar to that of a strong human (fewer than 2.5 pieces per second), and I’ll even force it to make mistakes from time to time (5% probability of making an error).\nI’ll tell you about the first approaches that yielded good results and served as the foundation for the ones that followed, which I’ll omit here to avoid making the article too long or overly technical or mathematical.\nI imagine the first thing you did was hit play and notice the surprising level achieved. It’s remarkable and quite a milestone for reinforcement learning. I managed to get an AI to learn effective strategies by itself, without receiving any advice; as you can see, it plans, can anticipate future situations without knowing the next pieces, performs combos, and doesn’t chase immediate reward.\nSo if the video already piqued your curiosity, I invite you to keep reading. Let’s go!\n","keywords":[],"articleBody":"Introduction In this second installment I’m going to explain how I tackled the challenge of training a Reinforcement Learning agent for Tetris, taking into account the difficulties I mentioned in the previous article.\nThe main goal is to reach a superhuman level—not in the sense that the agent plays faster or with greater precision than a person (that, after all, is expected from a machine), but because it becomes a better strategist. In fact, our agent will play at a speed similar to that of a strong human (fewer than 2.5 pieces per second), and I’ll even force it to make mistakes from time to time (5% probability of making an error).\nI’ll tell you about the first approaches that yielded good results and served as the foundation for the ones that followed, which I’ll omit here to avoid making the article too long or overly technical or mathematical.\nI imagine the first thing you did was hit play and notice the surprising level achieved. It’s remarkable and quite a milestone for reinforcement learning. I managed to get an AI to learn effective strategies by itself, without receiving any advice; as you can see, it plans, can anticipate future situations without knowing the next pieces, performs combos, and doesn’t chase immediate reward.\nSo if the video already piqued your curiosity, I invite you to keep reading. Let’s go!\nFirst steps The first step—which you might take for granted but is worth mentioning—consists of implementing a game environment that accurately replicates the behavior of a real match in tetr.io, since that will be the final setting where I want the agent to play.\nHowever, we don’t train directly in tetr.io. The main reason is that our environment needs to be fully optimized for speed so that matches can be executed as fast as possible.\nTo give you an idea, the environment I developed can simulate games hundreds of times faster than what we could achieve by playing directly in tetr.io.\nReward modeling As I mentioned in Part 1, one of the main challenges of applying Reinforcement Learning to Tetris is the design of the reward function. Games can be potentially infinite, and to train the agent properly it’s essential to be able to determine which game was played better.\nOne straightforward idea to address this would be to limit the number of pieces per game, for example, to 100. That way we could compare performance by final score, making the agent learn to maximize its score within that window.\nAlthough this initial approach is valid, it presents several issues:\nDifficulty growth.\nIn Tetris, achieving higher scores becomes exponentially more difficult as the game progresses. However, this linear reward model doesn’t reflect that complexity, and we want the agent to learn to recognize and adapt to that increasing difficulty.\nTemporal value of scores.\nIt may happen that one agent reaches a certain score quickly and then stalls, while another reaches the same score only at the end. If both receive the same final reward, the system will consider they performed equally, when in reality the first demonstrated a more efficient strategy at some points. Therefore, we must design the reward to favor score spikes or early high scores.\nEnd state of the game.\nAnother common case is that two games end with the same score, but one does so on the verge of topping out, while the other keeps the board in a stable situation. In that case, the basic model would consider both equally good, when the second clearly reflects more solid management. A simple way to address this is to vary the number of pieces per game: an agent that ends “on the brink of death” after 100 pieces would get a worse score when required to place 150, indirectly penalizing unstable strategies.\nEncouraging curiosity When someone faces a completely new situation—for example, a game they’ve never tried—the first thing they do is experiment: try different actions, observe the results, and learn from them. This trial-and-error process is essential to understand how the environment works.\nOur agent must do exactly the same. It’s not enough for it to repeat actions that yielded good results in the past; that would quickly lead to limited or biased learning. Instead, it must maintain an active curiosity, also exploring actions whose effects it doesn’t yet know.\nTo achieve this, I implemented an artificial curiosity mechanism via a second auxiliary neural network that runs in parallel to the main agent. This network has a very specific goal: to predict what will happen in the game when the agent takes a given action.\nFor example, if the agent decides to place a piece in an unusual position, the neural network tries to anticipate the outcome: how the board will look, how many lines will be cleared, whether holes will be created, etc.\nAfter the action is actually executed, we compare the network’s prediction with the real outcome.\nIf the prediction was very accurate, it means that action is already well known to the agent, and therefore further exploration need not be incentivized. But if the prediction fails or deviates significantly from what happened in practice, we interpret that the agent has discovered a new or underexplored situation. In that case, it is granted an additional intrinsic reward to motivate it to keep probing that kind of action. This mechanism essentially acts as a source of internal motivation: the agent feels “curiosity” when something surprises it and is drawn to keep trying until it understands it. Once it masters that situation, the curiosity fades and its behavior refocuses on maximizing the external reward (the game’s score).\nHowever, there’s a crucial aspect worth highlighting.\nIf the prediction network improves too much and ends up anticipating everything that can happen with precision, the agent stops being surprised by the environment. In practical terms, this means it loses curiosity, since all actions become predictable. As a result, its ability to continue discovering new or more effective strategies drops dramatically.\nTo avoid this stagnation, it’s necessary to introduce a kind of controlled forgetting into the network that generates curiosity. In other words, we make the network lose part of its knowledge from time to time, or we slightly degrade its predictive capacity over time. This may seem counterintuitive—why would we want a network to “forget”?—but it serves a very important purpose: to reactivate the agent’s curiosity toward situations it had previously explored.\nImagine this: at the start of training, the agent isn’t very skilled yet. When it faces a certain board configuration and experiments with a new action, it may explore it and satisfy its curiosity at that moment, but without really exploiting its full strategic potential, because it still doesn’t have the skills to do so.\nOver time, the agent improves, develops more advanced strategies, and learns to interpret the environment more sophisticatedly. At that point, those same situations it already knew may have new possibilities it previously wasn’t able to recognize.\nIf the curiosity network keeps its prior knowledge intact, it won’t consider those situations “interesting” again, and the agent will never return to them to rediscover them with its new perspective. That’s why it’s important for the network to partially “forget” what it learned: by doing so, it rekindles curiosity for those older patterns, allowing the agent to re-explore them with a higher level of competence.\nHierarchical agents In many video games, a player’s skills can be analyzed on two complementary levels: the macro-game and the micro-game.\nThe macro-game refers to the strategic dimension of the game: the ability to plan, anticipate events, and make decisions that determine the general course of the match. It answers “what to do” and “why to do it.”\nThe micro-game, on the other hand, describes the technical or execution dimension: the precision, speed, and coordination needed to effectively carry out the previously made strategic decisions. It answers “how to do it.”\nTranslating these concepts into the context of Tetris, the macro-game corresponds to global planning: deciding where to place each piece given the board state and, when possible, anticipating future pieces. The micro-game, in contrast, is reflected in the exact execution of that decision: how quickly the piece is rotated, the sequence of keys needed, and the precision with which it is finally placed in its ideal position.\nBoth levels are complementary: the most competent player combines a solid strategic vision with precise, efficient execution. However, they are two very different skills, both in how they are acquired and in the cognitive processes that support them.\nWith this distinction in mind, I chose to design two hierarchical AI agents, each specialized in one level:\nThe strategist agent (macro): responsible for deciding where the next piece should be placed based on the current board state. The executor agent (micro): responsible for translating that decision into concrete actions, i.e., computing which keys to press and in what order so the piece reaches the desired position. This hierarchical structure offers multiple advantages:\nMore efficient training.\nEach agent is trained to perform a very specific task, which simplifies the learning process and speeds up convergence.\nError separation.\nBy splitting responsibilities, we don’t punish one skill for the failures of the other. For example, if the strategist makes the best possible decision but the executor makes a mistake placing the piece, we don’t penalize the strategist for an error that isn’t its fault. This avoids confusing feedback and allows each agent to learn more stably.\nDifferent neural architectures.\nOne of the main benefits of splitting the system into two hierarchical agents is that we can design neural network architectures specialized for each task. Although both agents work with spatial information—so it’s natural to use convolutional neural networks (CNNs) to process the board state—their objectives and learning dynamics are so different that it doesn’t make sense to use the same architecture or hyperparameters for both.\nThe strategist agent, in charge of the macro-game, needs a deeper network with greater abstraction capacity, able to identify global patterns and long-range relationships across the board. In its case, it’s useful to incorporate attention layers or residual blocks that facilitate gradient flow and the understanding of complex spatial dependencies. It also tends to benefit from moderate regularization, such as mild dropout or light weight decay, to avoid overfitting without limiting exploration. As for hyperparameters, a larger batch size and smaller learning rates usually help stabilize training, since it is more sensitive to small variations in reward.\nThe executor agent, focused on the micro-game, requires a lighter, more reactive architecture optimized for speed and precision in decision-making. Here we prioritize shallower networks with fewer convolutional layers but denser connections, enabling immediate responses to environmental changes. It’s common to apply stricter regularizers, such as batch normalization and more aggressive dropout, to force generalization and prevent memorization of specific move sequences. Regarding hyperparameters, higher learning rates and training with frequent updates are advisable, favoring agile adaptation to game patterns.\nData augmentation addressed through attention As mentioned in Part 1, an effective way to help the agent recognize gameplay patterns is to expose it to different situations that share the same strategic solution. In Tetris, for example, two boards that look different may require exactly the same move if their structure is symmetric or equivalent in the upper area.\nA simple way to achieve this is via environment transformations, such as applying horizontal symmetries (horizontal flips) to both the board and the pieces. This effectively doubles the available examples, helping the agent generalize its strategies. However, I discovered an even more powerful and efficient alternative: introducing an attention mechanism over the board itself.\nThis attention system acts like a visual filter that limits how much information the strategist agent can process simultaneously. Specifically, the agent can focus on only four lines of the board at a time, which in most cases end up being—purely through training optimization—the top four lines, where the most critical decisions are made.\nThis constraint has a very interesting effect: the agent learns to abstract context and recognize equivalent patterns at different heights on the board. That is, it internalizes that the way to resolve a situation doesn’t depend on its absolute position, but on the local configuration of the pieces. In practice, this works like an implicit form of data augmentation, since the agent learns that many scenarios are essentially the same problem shifted vertically.\nThanks to this approach, the initial training process becomes much more efficient, because the agent needs to see fewer distinct examples to grasp general gameplay principles. In other words, it learns to “think by analogy”: a situation that once seemed unique comes to be seen as a variation of something it already masters.\nHowever, this attention system must evolve along with the agent. In the early phases, limiting the focus to just four lines makes learning easier by reducing perceptual complexity. But as the agent becomes more competent, that restriction turns into a limitation. To reach professional-level performance, the agent needs to progressively expand its attention field until it covers the entire board, integrating information from the lower lines to perform full-fledged planning.\nFighting a random environment Learning in a chaotic environment like a Tetris game is, literally, an epic challenge. As we mentioned in Part 1, the randomness of the pieces means no two games are identical in the entire history of the universe. The possible combinations of sequences and board configurations grow exponentially, reaching magnitudes that far exceed those of Go or Chess.\nThis level of entropy poses a fundamental problem for reinforcement learning: how can the agent learn stable, generalizable strategies if each game is essentially unique? Evaluating the value of a specific decision in such a changing context would require an astronomical number of simulations—totally unfeasible.\nThis is because the true value of a Tetris position doesn’t depend solely on the current board state, but also on the huge combinatorics of pieces that may appear next. Every possible future sequence drastically alters the validity of a present move, making it a problem of almost infinite temporal depth and combinatorics.\nTo overcome this obstacle, I developed a different strategy: instead of trying to battle randomness, I decided to tame it. The idea was to create a second agent, an “artificial rival,” whose role is not to compete directly by sending lines to the player—as in multiplayer Tetris—but to intervene in the distribution of pieces.\nHere’s how it works: this rival agent observes the board at each moment and selects, from all possible pieces, the one that is most unfavorable for the main player. In other words, it tries to offer the pieces that create the most challenging situations. Thus, we transform an originally random environment into a deterministic, adversarial one, in which every board configuration has its most challenging counterpart.\nBoth agents—the player and its rival—are trained in a parallel, co-evolutionary manner. As the player learns to solve increasingly difficult structures, the rival also improves, discovering new ways to break the player’s strategies. This process creates a constant feedback loop.\nThe result is progressive adversarial training in which both agents propel each other toward continuous improvement.\nWe are thus creating an AI player capable of handling the worst possible scenarios.\nAnd finally, when this agent faces a real game, sometimes it will encounter those same difficult situations it has already learned to handle, so it will know how to react and overcome them.\nAt other times it will run into more favorable scenarios, in which—thanks to its training under pressure—it will perform with even greater ease.\nResults As seen in the video, combining all the concepts described above made it possible to train a Tetris agent with a clearly professional level.\nIf you’re not familiar with competitive Tetris, its playstyle may seem odd: the goal for both expert players and the agent at this stage is not simply to clear lines, but to build tall, specific structures that enable advanced rotations (known as T-Spins and other variants). These maneuvers are more complex but yield a much higher score than conventional line clears.\nAt this point I can already state that a milestone in reinforcement learning has been reached: the agent has learned—without supervision or human examples—to play strategically and with foresight in an environment as unpredictable as Tetris. This result demonstrates the AI’s capacity to discover on its own tactical concepts and planning patterns traditionally attributed to human reasoning.\nHowever, even at such a high level, the goal remains to go further. The adversarial training strategy with the rival player has proven very effective, but there comes a time when improvement becomes extremely slow, either due to hardware limitations—in my case, an NVIDIA 3090 GPU with 24 GB—or because of the huge training time needed to keep progressing.\nIn the next parts of this series I’ll show how I moved forward from here, introducing new techniques and optimizations that enabled a virtually unattainable level of play.\n","wordCount":"2823","inLanguage":"en","image":"https://joan-projects.github.io/img/Tetris2.png","datePublished":"2025-11-05T00:00:00Z","dateModified":"2025-11-05T00:00:00Z","author":{"@type":"Person","name":"Joan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://joan-projects.github.io/en/posts/tetris2/"},"publisher":{"@type":"Organization","name":"Joan Projects","logo":{"@type":"ImageObject","url":"https://joan-projects.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://joan-projects.github.io/en/ accesskey=h title="Joan Projects (Alt + H)">Joan Projects</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://joan-projects.github.io/ title=Español aria-label=Español>Es</a></li></ul></div></div><ul id=menu><li><a href=https://joan-projects.github.io/en/ title=Home><span>Home</span></a></li><li><a href=https://joan-projects.github.io/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://joan-projects.github.io/en/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reinforcement Learning in Tetris – Part 2</h1><div class=post-meta><span title='2025-11-05 00:00:00 +0000 UTC'>November 5, 2025</span>&nbsp;·&nbsp;<span>14 min</span>&nbsp;·&nbsp;<span>Joan</span>&nbsp;|&nbsp;<span>Translations:</span><ul class=i18n_list><li><a href=https://joan-projects.github.io/posts/tetris2/>Es</a></li></ul></div></header><figure class="post-cover vc-single" data-vc-single data-controls=true data-autoplay=false data-loop=false data-muted=false style="position:relative;margin:0 0 1rem"><video class=vc-video preload=metadata playsinline poster=https://joan-projects.github.io/img/Tetris2.png style=width:100%;height:auto;display:block;border-radius:12px;object-fit:cover><source src=https://joan-projects.github.io/video/Tetris2.mp4 type=video/mp4>Tu navegador no soporta el elemento <code>video</code>.
</video>
<button type=button class=vc-play aria-label="Reproducir vídeo" style="position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);font-size:3rem;line-height:1;opacity:.9;background:rgba(0,0,0,.35);color:#fff;padding:.25em .5em;border-radius:.5em;border:0;cursor:pointer;z-index:3">
▶</button></figure><script>(function(){const e=document.currentScript.previousElementSibling;if(!e||!e.matches(".vc-single"))return;const t=e.querySelector("video"),n=e.querySelector(".vc-play"),i=e.dataset.controls==="true",a=e.dataset.autoplay==="true",r=e.dataset.loop==="true",c=e.dataset.muted==="true";function s(){n&&(n.style.display="none")}function o(){n&&(n.style.display="")}i&&(t.controls=!0),r&&(t.loop=!0),c&&(t.muted=!0),a&&s(),n&&n.addEventListener("click",function(e){e.preventDefault();const n=t.play();n&&typeof n.catch=="function"&&n.catch(()=>{}),s()}),t.addEventListener("playing",s),t.addEventListener("pause",o),t.addEventListener("ended",o)})()</script><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#first-steps aria-label="First steps">First steps</a></li><li><a href=#reward-modeling aria-label="Reward modeling">Reward modeling</a></li><li><a href=#encouraging-curiosity aria-label="Encouraging curiosity">Encouraging curiosity</a></li><li><a href=#hierarchical-agents aria-label="Hierarchical agents">Hierarchical agents</a></li><li><a href=#data-augmentation-addressed-through-attention aria-label="Data augmentation addressed through attention">Data augmentation addressed through attention</a></li><li><a href=#fighting-a-random-environment aria-label="Fighting a random environment">Fighting a random environment</a></li><li><a href=#results aria-label=Results>Results</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In this second installment I’m going to explain how I tackled the challenge of training a Reinforcement Learning agent for Tetris, taking into account the difficulties I mentioned in the previous article.<br>The main goal is to reach a superhuman level—not in the sense that the agent plays faster or with greater precision than a person (that, after all, is expected from a machine), but because it becomes a better strategist. In fact, our agent will play at a speed similar to that of a strong human (fewer than 2.5 pieces per second), and I’ll even force it to make mistakes from time to time (5% probability of making an error).<br>I’ll tell you about the first approaches that yielded good results and served as the foundation for the ones that followed, which I’ll omit here to avoid making the article too long or overly technical or mathematical.<br>I imagine the first thing you did was hit play and notice the surprising level achieved. It’s remarkable and quite a milestone for reinforcement learning. I managed to get an AI to learn effective strategies by itself, without receiving any advice; as you can see, it plans, can anticipate future situations without knowing the next pieces, performs combos, and doesn’t chase immediate reward.<br>So if the video already piqued your curiosity, I invite you to keep reading. Let’s go!</p><h2 id=first-steps>First steps<a hidden class=anchor aria-hidden=true href=#first-steps>#</a></h2><p>The first step—which you might take for granted but is worth mentioning—consists of implementing a game environment that accurately replicates the behavior of a real match in tetr.io, since that will be the final setting where I want the agent to play.<br>However, we don’t train directly in tetr.io. The main reason is that our environment needs to be fully optimized for speed so that matches can be executed as fast as possible.<br>To give you an idea, the environment I developed can simulate games hundreds of times faster than what we could achieve by playing directly in tetr.io.</p><h2 id=reward-modeling>Reward modeling<a hidden class=anchor aria-hidden=true href=#reward-modeling>#</a></h2><p>As I mentioned in Part 1, one of the main challenges of applying Reinforcement Learning to Tetris is the design of the reward function. Games can be potentially infinite, and to train the agent properly it’s essential to be able to determine which game was played better.<br>One straightforward idea to address this would be to limit the number of pieces per game, for example, to 100. That way we could compare performance by final score, making the agent learn to maximize its score within that window.</p><p>Although this initial approach is valid, it presents several issues:</p><ol><li><p><strong>Difficulty growth.</strong><br>In Tetris, achieving higher scores becomes exponentially more difficult as the game progresses. However, this linear reward model doesn’t reflect that complexity, and we want the agent to learn to recognize and adapt to that increasing difficulty.</p></li><li><p><strong>Temporal value of scores.</strong><br>It may happen that one agent reaches a certain score quickly and then stalls, while another reaches the same score only at the end. If both receive the same final reward, the system will consider they performed equally, when in reality the first demonstrated a more efficient strategy at some points. Therefore, we must design the reward to favor score spikes or early high scores.</p></li><li><p><strong>End state of the game.</strong><br>Another common case is that two games end with the same score, but one does so on the verge of topping out, while the other keeps the board in a stable situation. In that case, the basic model would consider both equally good, when the second clearly reflects more solid management. A simple way to address this is to vary the number of pieces per game: an agent that ends “on the brink of death” after 100 pieces would get a worse score when required to place 150, indirectly penalizing unstable strategies.</p></li></ol><h2 id=encouraging-curiosity>Encouraging curiosity<a hidden class=anchor aria-hidden=true href=#encouraging-curiosity>#</a></h2><p>When someone faces a completely new situation—for example, a game they’ve never tried—the first thing they do is experiment: try different actions, observe the results, and learn from them. This trial-and-error process is essential to understand how the environment works.<br>Our agent must do exactly the same. It’s not enough for it to repeat actions that yielded good results in the past; that would quickly lead to limited or biased learning. Instead, it must maintain an active curiosity, also exploring actions whose effects it doesn’t yet know.</p><p>To achieve this, I implemented an artificial curiosity mechanism via a second auxiliary neural network that runs in parallel to the main agent. This network has a very specific goal: to predict what will happen in the game when the agent takes a given action.<br>For example, if the agent decides to place a piece in an unusual position, the neural network tries to anticipate the outcome: how the board will look, how many lines will be cleared, whether holes will be created, etc.<br>After the action is actually executed, we compare the network’s prediction with the real outcome.</p><ul><li>If the prediction was very accurate, it means that action is already well known to the agent, and therefore further exploration need not be incentivized.</li><li>But if the prediction fails or deviates significantly from what happened in practice, we interpret that the agent has discovered a new or underexplored situation. In that case, it is granted an additional <strong>intrinsic reward</strong> to motivate it to keep probing that kind of action.</li></ul><p>This mechanism essentially acts as a source of internal motivation: the agent feels “curiosity” when something surprises it and is drawn to keep trying until it understands it. Once it masters that situation, the curiosity fades and its behavior refocuses on maximizing the external reward (the game’s score).</p><p>However, there’s a crucial aspect worth highlighting.<br>If the prediction network improves too much and ends up anticipating everything that can happen with precision, the agent stops being surprised by the environment. In practical terms, this means it loses curiosity, since all actions become predictable. As a result, its ability to continue discovering new or more effective strategies drops dramatically.<br>To avoid this stagnation, it’s necessary to introduce a kind of <strong>controlled forgetting</strong> into the network that generates curiosity. In other words, we make the network lose part of its knowledge from time to time, or we slightly degrade its predictive capacity over time. This may seem counterintuitive—why would we want a network to “forget”?—but it serves a very important purpose: to reactivate the agent’s curiosity toward situations it had previously explored.</p><p>Imagine this: at the start of training, the agent isn’t very skilled yet. When it faces a certain board configuration and experiments with a new action, it may explore it and satisfy its curiosity at that moment, but without really exploiting its full strategic potential, because it still doesn’t have the skills to do so.<br>Over time, the agent improves, develops more advanced strategies, and learns to interpret the environment more sophisticatedly. At that point, those same situations it already knew may have new possibilities it previously wasn’t able to recognize.<br>If the curiosity network keeps its prior knowledge intact, it won’t consider those situations “interesting” again, and the agent will never return to them to rediscover them with its new perspective. That’s why it’s important for the network to partially “forget” what it learned: by doing so, it rekindles curiosity for those older patterns, allowing the agent to re-explore them with a higher level of competence.</p><h2 id=hierarchical-agents>Hierarchical agents<a hidden class=anchor aria-hidden=true href=#hierarchical-agents>#</a></h2><p>In many video games, a player’s skills can be analyzed on two complementary levels: the macro-game and the micro-game.<br>The macro-game refers to the strategic dimension of the game: the ability to plan, anticipate events, and make decisions that determine the general course of the match. It answers “what to do” and “why to do it.”<br>The micro-game, on the other hand, describes the technical or execution dimension: the precision, speed, and coordination needed to effectively carry out the previously made strategic decisions. It answers “how to do it.”</p><p>Translating these concepts into the context of Tetris, the macro-game corresponds to global planning: deciding where to place each piece given the board state and, when possible, anticipating future pieces. The micro-game, in contrast, is reflected in the exact execution of that decision: how quickly the piece is rotated, the sequence of keys needed, and the precision with which it is finally placed in its ideal position.<br>Both levels are complementary: the most competent player combines a solid strategic vision with precise, efficient execution. However, they are two very different skills, both in how they are acquired and in the cognitive processes that support them.</p><p>With this distinction in mind, I chose to design two hierarchical AI agents, each specialized in one level:</p><ul><li><strong>The strategist agent (macro):</strong> responsible for deciding where the next piece should be placed based on the current board state.</li><li><strong>The executor agent (micro):</strong> responsible for translating that decision into concrete actions, i.e., computing which keys to press and in what order so the piece reaches the desired position.</li></ul><p>This hierarchical structure offers multiple advantages:</p><ol><li><p><strong>More efficient training.</strong><br>Each agent is trained to perform a very specific task, which simplifies the learning process and speeds up convergence.</p></li><li><p><strong>Error separation.</strong><br>By splitting responsibilities, we don’t punish one skill for the failures of the other. For example, if the strategist makes the best possible decision but the executor makes a mistake placing the piece, we don’t penalize the strategist for an error that isn’t its fault. This avoids confusing feedback and allows each agent to learn more stably.</p></li><li><p><strong>Different neural architectures.</strong><br>One of the main benefits of splitting the system into two hierarchical agents is that we can design neural network architectures specialized for each task. Although both agents work with spatial information—so it’s natural to use convolutional neural networks (CNNs) to process the board state—their objectives and learning dynamics are so different that it doesn’t make sense to use the same architecture or hyperparameters for both.<br>The strategist agent, in charge of the macro-game, needs a deeper network with greater abstraction capacity, able to identify global patterns and long-range relationships across the board. In its case, it’s useful to incorporate attention layers or residual blocks that facilitate gradient flow and the understanding of complex spatial dependencies. It also tends to benefit from moderate regularization, such as mild dropout or light weight decay, to avoid overfitting without limiting exploration. As for hyperparameters, a larger batch size and smaller learning rates usually help stabilize training, since it is more sensitive to small variations in reward.<br>The executor agent, focused on the micro-game, requires a lighter, more reactive architecture optimized for speed and precision in decision-making. Here we prioritize shallower networks with fewer convolutional layers but denser connections, enabling immediate responses to environmental changes. It’s common to apply stricter regularizers, such as batch normalization and more aggressive dropout, to force generalization and prevent memorization of specific move sequences. Regarding hyperparameters, higher learning rates and training with frequent updates are advisable, favoring agile adaptation to game patterns.</p></li></ol><h2 id=data-augmentation-addressed-through-attention>Data augmentation addressed through attention<a hidden class=anchor aria-hidden=true href=#data-augmentation-addressed-through-attention>#</a></h2><p>As mentioned in Part 1, an effective way to help the agent recognize gameplay patterns is to expose it to different situations that share the same strategic solution. In Tetris, for example, two boards that look different may require exactly the same move if their structure is symmetric or equivalent in the upper area.<br>A simple way to achieve this is via environment transformations, such as applying horizontal symmetries (horizontal flips) to both the board and the pieces. This effectively doubles the available examples, helping the agent generalize its strategies. However, I discovered an even more powerful and efficient alternative: introducing an attention mechanism over the board itself.</p><p>This attention system acts like a visual filter that limits how much information the strategist agent can process simultaneously. Specifically, the agent can focus on only four lines of the board at a time, which in most cases end up being—purely through training optimization—the top four lines, where the most critical decisions are made.<br>This constraint has a very interesting effect: the agent learns to abstract context and recognize equivalent patterns at different heights on the board. That is, it internalizes that the way to resolve a situation doesn’t depend on its absolute position, but on the local configuration of the pieces. In practice, this works like an implicit form of data augmentation, since the agent learns that many scenarios are essentially the same problem shifted vertically.</p><p>Thanks to this approach, the initial training process becomes much more efficient, because the agent needs to see fewer distinct examples to grasp general gameplay principles. In other words, it learns to “think by analogy”: a situation that once seemed unique comes to be seen as a variation of something it already masters.<br>However, this attention system must evolve along with the agent. In the early phases, limiting the focus to just four lines makes learning easier by reducing perceptual complexity. But as the agent becomes more competent, that restriction turns into a limitation. To reach professional-level performance, the agent needs to progressively expand its attention field until it covers the entire board, integrating information from the lower lines to perform full-fledged planning.</p><h2 id=fighting-a-random-environment>Fighting a random environment<a hidden class=anchor aria-hidden=true href=#fighting-a-random-environment>#</a></h2><p>Learning in a chaotic environment like a Tetris game is, literally, an epic challenge. As we mentioned in Part 1, the randomness of the pieces means no two games are identical in the entire history of the universe. The possible combinations of sequences and board configurations grow exponentially, reaching magnitudes that far exceed those of Go or Chess.<br>This level of entropy poses a fundamental problem for reinforcement learning: how can the agent learn stable, generalizable strategies if each game is essentially unique? Evaluating the value of a specific decision in such a changing context would require an astronomical number of simulations—totally unfeasible.</p><p>This is because the true value of a Tetris position doesn’t depend solely on the current board state, but also on the huge combinatorics of pieces that may appear next. Every possible future sequence drastically alters the validity of a present move, making it a problem of almost infinite temporal depth and combinatorics.<br>To overcome this obstacle, I developed a different strategy: instead of trying to battle randomness, I decided to tame it. The idea was to create a second agent, an “artificial rival,” whose role is not to compete directly by sending lines to the player—as in multiplayer Tetris—but to intervene in the distribution of pieces.<br>Here’s how it works: this rival agent observes the board at each moment and selects, from all possible pieces, the one that is most unfavorable for the main player. In other words, it tries to offer the pieces that create the most challenging situations. Thus, we transform an originally random environment into a deterministic, adversarial one, in which every board configuration has its most challenging counterpart.</p><p>Both agents—the player and its rival—are trained in a parallel, co-evolutionary manner. As the player learns to solve increasingly difficult structures, the rival also improves, discovering new ways to break the player’s strategies. This process creates a constant feedback loop.<br>The result is progressive adversarial training in which both agents propel each other toward continuous improvement.<br>We are thus creating an AI player capable of handling the worst possible scenarios.<br>And finally, when this agent faces a real game, sometimes it will encounter those same difficult situations it has already learned to handle, so it will know how to react and overcome them.<br>At other times it will run into more favorable scenarios, in which—thanks to its training under pressure—it will perform with even greater ease.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>As seen in the video, combining all the concepts described above made it possible to train a Tetris agent with a clearly professional level.<br>If you’re not familiar with competitive Tetris, its playstyle may seem odd: the goal for both expert players and the agent at this stage is not simply to clear lines, but to build tall, specific structures that enable advanced rotations (known as T-Spins and other variants). These maneuvers are more complex but yield a much higher score than conventional line clears.<br>At this point I can already state that a milestone in reinforcement learning has been reached: the agent has learned—without supervision or human examples—to play strategically and with foresight in an environment as unpredictable as Tetris. This result demonstrates the AI’s capacity to discover on its own tactical concepts and planning patterns traditionally attributed to human reasoning.<br>However, even at such a high level, the goal remains to go further. The adversarial training strategy with the rival player has proven very effective, but there comes a time when improvement becomes extremely slow, either due to hardware limitations—in my case, an NVIDIA 3090 GPU with 24 GB—or because of the huge training time needed to keep progressing.<br>In the next parts of this series I’ll show how I moved forward from here, introducing new techniques and optimizations that enabled a virtually unattainable level of play.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://joan-projects.github.io/en/posts/tetris1/><span class=title>Next »</span><br><span>Machine Learning in Tetris – Part 1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on x" href="https://x.com/intent/tweet/?text=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f&amp;title=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202&amp;summary=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202&amp;source=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f&title=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on whatsapp" href="https://api.whatsapp.com/send?text=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202%20-%20https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on telegram" href="https://telegram.me/share/url?text=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement Learning in Tetris – Part 2 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Reinforcement%20Learning%20in%20Tetris%20%e2%80%93%20Part%202&u=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://joan-projects.github.io/en/>Joan Projects</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>window.addEventListener("load",function(){if(window.renderMathInElement){const e=document.querySelector("article, .post-content, main, body");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})}})</script><script>(function(){if(window.__VC_LIST_READY__)return;window.__VC_LIST_READY__=!0;function e(e){document.querySelectorAll(".vc-entry video").forEach(t=>{t!==e&&t.pause()})}document.addEventListener("click",function(t){const s=t.target.closest(".vc-poster");if(!s)return;t.preventDefault();const i=s.dataset.mp4,a=s.dataset.webm,r=s.dataset.poster,c=s.dataset.controls==="true",u=s.dataset.autoplay==="true",l=s.dataset.loop==="true",d=s.dataset.muted==="true",n=document.createElement("video");if(n.preload="metadata",n.playsInline=!0,r&&(n.poster=r),c&&(n.controls=!0),l&&(n.loop=!0),d&&(n.muted=!0),a){const e=document.createElement("source");e.src=a,e.type="video/webm",n.appendChild(e)}if(i){const e=document.createElement("source");e.src=i,e.type="video/mp4",n.appendChild(e)}const h=s.parentElement;s.replaceWith(n),e(n);const o=n.play();o&&typeof o.catch=="function"&&o.catch(()=>{})},!1)})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>