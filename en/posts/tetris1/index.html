<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning in Tetris – Part 1 | Joan Projects</title><meta name=keywords content><meta name=description content="Introduction
Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.
Explanatory note:
In this document I use the term &ldquo;machine learning&rdquo; to make it easier for non-technical readers to understand.
However, it is important to note that the techniques described correspond specifically to reinforcement learning methods.
The reason for this terminological choice is that the concept of machine learning more intuitively conveys the idea that the training process is carried out without direct human intervention, that is, without providing explicit instructions or steering the strategy the system should learn.
In other words, although the general term is &ldquo;machine learning,&rdquo; the actual approach is based on agents that learn on their own through interaction with their environment, optimizing their behavior through rewards or penalties."><meta name=author content="Joan"><link rel=canonical href=https://joan-projects.github.io/en/posts/tetris1/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://joan-projects.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://joan-projects.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://joan-projects.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://joan-projects.github.io/apple-touch-icon.png><link rel=mask-icon href=https://joan-projects.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=es href=https://joan-projects.github.io/posts/tetris1/><link rel=alternate hreflang=en href=https://joan-projects.github.io/en/posts/tetris1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js crossorigin=anonymous></script><style>.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><style>.vc-entry{position:relative}.vc-poster{position:relative;display:block;width:100%;padding:0;border:0;background:0 0;cursor:pointer}.vc-poster .entry-cover-image{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-entry video{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-play{position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);font-size:3rem;line-height:1;opacity:.9;background:rgba(0,0,0,.35);color:#fff;padding:.25em .5em;border-radius:.5em}@media(max-width:640px){.vc-play{font-size:2.2rem}}.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><meta property="og:url" content="https://joan-projects.github.io/en/posts/tetris1/"><meta property="og:site_name" content="Joan Projects"><meta property="og:title" content="Machine Learning in Tetris – Part 1"><meta property="og:description" content="Introduction Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.
Explanatory note:
In this document I use the term “machine learning” to make it easier for non-technical readers to understand.
However, it is important to note that the techniques described correspond specifically to reinforcement learning methods.
The reason for this terminological choice is that the concept of machine learning more intuitively conveys the idea that the training process is carried out without direct human intervention, that is, without providing explicit instructions or steering the strategy the system should learn.
In other words, although the general term is “machine learning,” the actual approach is based on agents that learn on their own through interaction with their environment, optimizing their behavior through rewards or penalties."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-22T00:00:00+00:00"><meta property="og:image" content="https://joan-projects.github.io/img/Tetris1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://joan-projects.github.io/img/Tetris1.png"><meta name=twitter:title content="Machine Learning in Tetris – Part 1"><meta name=twitter:description content="Introduction
Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.
Explanatory note:
In this document I use the term &ldquo;machine learning&rdquo; to make it easier for non-technical readers to understand.
However, it is important to note that the techniques described correspond specifically to reinforcement learning methods.
The reason for this terminological choice is that the concept of machine learning more intuitively conveys the idea that the training process is carried out without direct human intervention, that is, without providing explicit instructions or steering the strategy the system should learn.
In other words, although the general term is &ldquo;machine learning,&rdquo; the actual approach is based on agents that learn on their own through interaction with their environment, optimizing their behavior through rewards or penalties."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://joan-projects.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"Machine Learning in Tetris – Part 1","item":"https://joan-projects.github.io/en/posts/tetris1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Machine Learning in Tetris – Part 1","name":"Machine Learning in Tetris – Part 1","description":"Introduction Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.\nThroughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.\nExplanatory note:\nIn this document I use the term \u0026ldquo;machine learning\u0026rdquo; to make it easier for non-technical readers to understand.\nHowever, it is important to note that the techniques described correspond specifically to reinforcement learning methods.\nThe reason for this terminological choice is that the concept of machine learning more intuitively conveys the idea that the training process is carried out without direct human intervention, that is, without providing explicit instructions or steering the strategy the system should learn.\nIn other words, although the general term is \u0026ldquo;machine learning,\u0026rdquo; the actual approach is based on agents that learn on their own through interaction with their environment, optimizing their behavior through rewards or penalties.\n","keywords":[],"articleBody":"Introduction Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.\nThroughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.\nExplanatory note:\nIn this document I use the term “machine learning” to make it easier for non-technical readers to understand.\nHowever, it is important to note that the techniques described correspond specifically to reinforcement learning methods.\nThe reason for this terminological choice is that the concept of machine learning more intuitively conveys the idea that the training process is carried out without direct human intervention, that is, without providing explicit instructions or steering the strategy the system should learn.\nIn other words, although the general term is “machine learning,” the actual approach is based on agents that learn on their own through interaction with their environment, optimizing their behavior through rewards or penalties.\nTetris: the simple game that challenges machine learning In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.\nBut there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.\nYes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.\nLet’s see why.\nThe overwhelming number of possibilities To understand the magnitude of the challenge, it’s enough to compare the number of possible situations a player can encounter in each game.\nIn Chess, the number of possible board configurations is around 4,82 × 10⁴⁴. In Go, it skyrockets to 2,08 × 10¹⁷⁰.\nIn comparison, Tetris plays in a different league.\nA Tetris board of 20 rows by 10 columns already offers $2^{20\\times10}$ possible configurations, but that’s only the beginning.\nIf we add the random piece bag factor, and limit ourselves to a short game of about 200 pieces (approximately one minute of play), the total number of possibilities scales to the inconceivable:\n(1,37 × 10³⁷¹) × 2²⁰⁰ ≈ 2,2 × 10⁴³¹\nIn other words, Tetris’s state space is astronomically larger than that of any other classic game.\nTetris is not deterministic: the chaos of randomness And yet, the size of the state space is not the biggest problem.\nThe real difficulty lies in the fact that Tetris is not a deterministic game.\nIn Chess or Go, a specific move always produces the same result. The rules are fixed and the consequences, predictable.\nIn Tetris, however, a decision can be excellent or disastrous depending on which piece comes next.\nThe context is constantly changing and the environment is highly unstable.\nThis means that the evaluation of a position cannot depend only on the current board: the immediate future—unpredictable by nature—changes the value of every action.\nAs a consequence, the well-known MCTS (Monte Carlo Tree Search) method, which was the cornerstone of AlphaZero and AlphaGo, proves ineffective for Tetris. Its structure depends on deterministic scenarios with repeatable outcomes, the exact opposite of Tetris chaos.\nReward, number of actions, and planning The contrast widens when we look at the decision dynamics:\nIn Chess, a player makes about 40 decisions per game. In Go, around 100. In Tetris, by contrast, the number of actions is potentially infinite. And whereas in Chess or Go the final reward is clear—win, draw, or loss—in Tetris the reward can be defined as a score… but with no known upper bound.\nThis makes the reward function an AI must learn enormously complicated.\nAn AI can have a good run, but how does it know if its score is “good” or “excellent”?\nThis lack of reference complicates the design of the reward function, a crucial element in reinforcement learning.\nThe data-gathering problem For an artificial intelligence to truly learn to play Tetris, it’s not enough to show it a few games: it has to play thousands, millions, even hundreds of millions of times.\nOnly then can it begin to grasp the game’s patterns and, little by little, discover for itself the most effective strategies.\nFortunately, computers play infinitely faster than humans. While a person can play a few games in an hour, an AI agent can play millions in the same time, learning from every mistake and every success.\nThe key in this process is to record what happens at every moment: what situation the agent sees, what action it takes, and what result it gets.\nEach of these records is called a transition.\nEach transition is, in essence, a small lesson that the AI stores in its memory to learn how to act the next time it faces something similar.\nTetris versus Chess and Go In Chess, a transition is very quick to compute: in most cases we only need to check whether the move is legal or not; by contrast, in Tetris, not only that, but it is necessary to compute how that move affects the board and build the new piece bag.\nTo be more precise, performing a transition in Chess can be hundreds of times faster than in Tetris.\nTherefore, the total time to obtain training data is much greater.\nHowever, compared to Go, the time per transition is similar.\nNevertheless—and here’s the catch—simulation speed isn’t everything.\nAlthough computers seem capable of playing at unimaginable speeds, the resources of a home computer are not enough to train a complex AI of this kind.\nThe problem is not just computational power, but the time it takes to obtain truly useful data.\nWhy data collection time matters In games like Chess or Go, consequences usually become apparent quite quickly, normally in the next transition. In contrast, in Tetris, when playing at an advanced level, they may become noticeable even more than ten transitions later. That’s why a bad move can look good for several iterations until its consequences show up later.\nThis forces the agent to maintain a “sense of the future”: a memory that links past decisions with delayed outcomes. But how to do this in Tetris? We don’t know which pieces are going to arrive, so what the player sees at any given moment is not easily evaluable.\nAnd if we also want the system to simulate different alternatives before deciding—like the MCTS (Monte Carlo Tree Search) algorithm does—compute time grows exponentially.\nYou can imagine it like a chess player mentally analyzing three, four, or five moves ahead before moving a piece.\nNow imagine they have to do it millions of times per second: that’s the difficulty.\nData augmentation: multiplying experience without playing more A very powerful technique to accelerate learning is data augmentation (data augmentation).\nIt consists of creating new examples from those we already have, modifying them so that they remain valid but add variety to the training.\nA classic example comes from Chess:\nimagine an endgame with two rooks trying to checkmate a king.\nIf the enemy king is in the upper-right corner of the board and we achieve mate in three moves, that same sequence is exactly equivalent by symmetry if the king is in any of the other three corners.\nThat is, from a single datum we get four, almost for free.\nIn Chess, almost all positions can be duplicated (×2), and most can be quadrupled (×4). In Go, thanks to its symmetries and rotations, we can do an ×8 of each data point. In Tetris, however, the situation is less favorable: the board has a single possible orientation (gravity always acts downward), so we can only double (×2) the available data. This means that Tetris starts with an additional handicap: it not only needs more data, but it can also generate them more slowly and take less advantage of augmentation.\nWhy augmentation is essential Data augmentation not only multiplies the amount of available information, it also teaches the system to recognize fundamental patterns.\nBy seeing equivalent situations from different perspectives, the AI learns to generalize better, to understand which features of the board are truly important and which are circumstantial.\nIt’s like teaching a dog to recognize a thief: if it has only seen one dressed in black, it will think all thieves wear black.\nBut if it has seen thieves with jackets, hats, or shirts of different colors, it will learn that what matters is not the color of the clothing, but the fact that they’re taking objects from the owner’s house.\nHeuristics vs. machine learning: two different paths Before the era of machine learning, as in Chess, surprising results were achieved in Tetris using heuristic methods.\nThe idea was simple: simulate all possible positions after placing one or several pieces and evaluate each board with a mathematical function designed by humans.\nFor example, a typical heuristic penalizes holes, rewards complete lines, and seeks to minimize the average height of the columns.\nThe program generates hundreds or thousands of possible boards and chooses the move that produces the highest value according to that function.\nThe problem is that a heuristic doesn’t really “think.”\nIt only executes a fixed formula, written by a developer.\nAll the “intelligence” resides in the evaluation function, not in the system.\nThis means the program will never learn anything new: if the creator doesn’t know the best strategy, the AI won’t discover it either.\nMachine learning, by contrast, represents a qualitative leap.\nIt allows us to tackle problems in which the optimal strategy is unknown, because the system itself will discover it during training.\nThrough thousands of games, trial and error, and value adjustments, the AI learns by itself which patterns lead to success, even in environments as chaotic and unpredictable as Tetris.\nIt doesn’t need us to tell it which features make a position good: it eventually infers them.\nThat ability to generate knowledge without direct human intervention is what makes machine learning—and especially reinforcement learning—a revolutionary tool.\nMoreover, the heuristic approach cannot plan strategies several moves deep.\nThis is because the number of configurations grows exponentially with each added piece (in Tetris, on the order of $162^{n}$).\nIf the algorithm tried to foresee the result of, say, five future pieces, it would need to explore $162^{5}≈10^{11}$ — an impossible task in real time.\nTherefore, heuristics live in the present: they can only optimize the immediate move.\nMachine learning, by contrast, can anticipate the future by incorporating the expected long-term reward into its training process.\nThis allows it to develop sustainable strategies that not only seek immediate gain (such as clearing a line), but also keep the board stable and adaptable for the upcoming pieces.\nClosing Now that we’ve established the main challenges a machine learning agent faces in Tetris, in the next chapters we’ll explore step by step how my system evolved until reaching the coveted goal:\nan AI capable of playing Tetris at the highest possible level.\n","wordCount":"1865","inLanguage":"en","image":"https://joan-projects.github.io/img/Tetris1.png","datePublished":"2025-10-22T00:00:00Z","dateModified":"2025-10-22T00:00:00Z","author":{"@type":"Person","name":"Joan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://joan-projects.github.io/en/posts/tetris1/"},"publisher":{"@type":"Organization","name":"Joan Projects","logo":{"@type":"ImageObject","url":"https://joan-projects.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://joan-projects.github.io/en/ accesskey=h title="Joan Projects (Alt + H)">Joan Projects</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://joan-projects.github.io/ title=Español aria-label=Español>Es</a></li></ul></div></div><ul id=menu><li><a href=https://joan-projects.github.io/en/ title=Home><span>Home</span></a></li><li><a href=https://joan-projects.github.io/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://joan-projects.github.io/en/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Machine Learning in Tetris – Part 1</h1><div class=post-meta><span title='2025-10-22 00:00:00 +0000 UTC'>October 22, 2025</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>Joan</span>&nbsp;|&nbsp;<span>Translations:</span><ul class=i18n_list><li><a href=https://joan-projects.github.io/posts/tetris1/>Es</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://joan-projects.github.io/img/Tetris1.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#tetris-the-simple-game-that-challenges-machine-learning aria-label="Tetris: the simple game that challenges machine learning">Tetris: the simple game that challenges machine learning</a></li><li><a href=#the-overwhelming-number-of-possibilities aria-label="The overwhelming number of possibilities">The overwhelming number of possibilities</a></li><li><a href=#tetris-is-not-deterministic-the-chaos-of-randomness aria-label="Tetris is not deterministic: the chaos of randomness">Tetris is not deterministic: the chaos of randomness</a></li><li><a href=#reward-number-of-actions-and-planning aria-label="Reward, number of actions, and planning">Reward, number of actions, and planning</a></li><li><a href=#the-data-gathering-problem aria-label="The data-gathering problem">The data-gathering problem</a></li><li><a href=#tetris-versus-chess-and-go aria-label="Tetris versus Chess and Go">Tetris versus Chess and Go</a></li><li><a href=#why-data-collection-time-matters aria-label="Why data collection time matters">Why data collection time matters</a></li><li><a href=#data-augmentation-multiplying-experience-without-playing-more aria-label="Data augmentation: multiplying experience without playing more">Data augmentation: multiplying experience without playing more</a></li><li><a href=#why-augmentation-is-essential aria-label="Why augmentation is essential">Why augmentation is essential</a></li><li><a href=#heuristics-vs-machine-learning-two-different-paths aria-label="Heuristics vs. machine learning: two different paths">Heuristics vs. machine learning: two different paths</a></li><li><a href=#closing aria-label=Closing>Closing</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.<br>Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.</p><p><strong>Explanatory note:</strong><br>In this document I use the term <strong>&ldquo;machine learning&rdquo;</strong> to make it easier for non-technical readers to understand.<br>However, it is important to note that the techniques described correspond specifically to <strong>reinforcement learning methods</strong>.<br>The reason for this terminological choice is that the concept of <em>machine learning</em> more intuitively conveys the idea that the training process is carried out <strong>without direct human intervention</strong>, that is, without providing explicit instructions or steering the strategy the system should learn.<br>In other words, although the general term is &ldquo;machine learning,&rdquo; the actual approach is based on <strong>agents that learn on their own through interaction with their environment</strong>, optimizing their behavior through rewards or penalties.</p><h2 id=tetris-the-simple-game-that-challenges-machine-learning>Tetris: the simple game that challenges machine learning<a hidden class=anchor aria-hidden=true href=#tetris-the-simple-game-that-challenges-machine-learning>#</a></h2><p>In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.<br>But there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.<br>Yes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.</p><p>Let’s see why.</p><h2 id=the-overwhelming-number-of-possibilities>The overwhelming number of possibilities<a hidden class=anchor aria-hidden=true href=#the-overwhelming-number-of-possibilities>#</a></h2><p>To understand the magnitude of the challenge, it’s enough to compare the number of possible situations a player can encounter in each game.<br>In Chess, the number of possible board configurations is around 4,82 × 10⁴⁴. In Go, it skyrockets to 2,08 × 10¹⁷⁰.<br>In comparison, Tetris plays in a different league.<br>A Tetris board of 20 rows by 10 columns already offers $2^{20\times10}$
possible configurations, but that’s only the beginning.<br>If we add the random piece bag factor, and limit ourselves to a short game of about 200 pieces (approximately one minute of play), the total number of possibilities scales to the inconceivable:</p><p>(1,37 × 10³⁷¹) × 2²⁰⁰ ≈ 2,2 × 10⁴³¹</p><p>In other words, Tetris’s state space is astronomically larger than that of any other classic game.</p><h2 id=tetris-is-not-deterministic-the-chaos-of-randomness>Tetris is not deterministic: the chaos of randomness<a hidden class=anchor aria-hidden=true href=#tetris-is-not-deterministic-the-chaos-of-randomness>#</a></h2><p>And yet, the size of the state space is not the biggest problem.<br>The real difficulty lies in the fact that Tetris is not a deterministic game.<br>In Chess or Go, a specific move always produces the same result. The rules are fixed and the consequences, predictable.<br>In Tetris, however, a decision can be excellent or disastrous depending on which piece comes next.<br>The context is constantly changing and the environment is highly unstable.<br>This means that the evaluation of a position cannot depend only on the current board: the immediate future—unpredictable by nature—changes the value of every action.<br>As a consequence, the well-known MCTS (Monte Carlo Tree Search) method, which was the cornerstone of AlphaZero and AlphaGo, proves ineffective for Tetris. Its structure depends on deterministic scenarios with repeatable outcomes, the exact opposite of Tetris chaos.</p><h2 id=reward-number-of-actions-and-planning>Reward, number of actions, and planning<a hidden class=anchor aria-hidden=true href=#reward-number-of-actions-and-planning>#</a></h2><p>The contrast widens when we look at the decision dynamics:</p><ul><li>In Chess, a player makes about 40 decisions per game.</li><li>In Go, around 100.</li><li>In Tetris, by contrast, the number of actions is potentially infinite.</li></ul><p>And whereas in Chess or Go the final reward is clear—win, draw, or loss—in Tetris the reward can be defined as a score… but with no known upper bound.<br>This makes the reward function an AI must learn enormously complicated.<br>An AI can have a good run, but how does it know if its score is “good” or “excellent”?<br>This lack of reference complicates the design of the reward function, a crucial element in reinforcement learning.</p><h2 id=the-data-gathering-problem>The data-gathering problem<a hidden class=anchor aria-hidden=true href=#the-data-gathering-problem>#</a></h2><p>For an artificial intelligence to truly learn to play Tetris, it’s not enough to show it a few games: it has to play thousands, millions, even hundreds of millions of times.<br>Only then can it begin to grasp the game’s patterns and, little by little, discover for itself the most effective strategies.<br>Fortunately, computers play infinitely faster than humans. While a person can play a few games in an hour, an AI agent can play millions in the same time, learning from every mistake and every success.<br>The key in this process is to record what happens at every moment: what situation the agent sees, what action it takes, and what result it gets.<br>Each of these records is called a <strong>transition</strong>.<br>Each transition is, in essence, a small lesson that the AI stores in its memory to learn how to act the next time it faces something similar.</p><h2 id=tetris-versus-chess-and-go>Tetris versus Chess and Go<a hidden class=anchor aria-hidden=true href=#tetris-versus-chess-and-go>#</a></h2><p>In Chess, a transition is very quick to compute: in most cases we only need to check whether the move is legal or not; by contrast, in Tetris, not only that, but it is necessary to compute how that move affects the board and build the new piece bag.<br>To be more precise, performing a transition in Chess can be hundreds of times faster than in Tetris.<br>Therefore, the total time to obtain training data is much greater.<br>However, compared to Go, the time per transition is similar.<br>Nevertheless—and here’s the catch—simulation speed isn’t everything.<br>Although computers seem capable of playing at unimaginable speeds, the resources of a home computer are not enough to train a complex AI of this kind.<br>The problem is not just computational power, but the time it takes to obtain truly useful data.</p><h2 id=why-data-collection-time-matters>Why data collection time matters<a hidden class=anchor aria-hidden=true href=#why-data-collection-time-matters>#</a></h2><p>In games like Chess or Go, consequences usually become apparent quite quickly, normally in the next transition. In contrast, in Tetris, when playing at an advanced level, they may become noticeable even more than ten transitions later.
That’s why a bad move can look good for several iterations until its consequences show up later.<br>This forces the agent to maintain a “sense of the future”: a memory that links past decisions with delayed outcomes.
But how to do this in Tetris? We don’t know which pieces are going to arrive, so what the player sees at any given moment is not easily evaluable.<br>And if we also want the system to simulate different alternatives before deciding—like the MCTS (Monte Carlo Tree Search) algorithm does—compute time grows exponentially.<br>You can imagine it like a chess player mentally analyzing three, four, or five moves ahead before moving a piece.<br>Now imagine they have to do it millions of times per second: that’s the difficulty.</p><h2 id=data-augmentation-multiplying-experience-without-playing-more>Data augmentation: multiplying experience without playing more<a hidden class=anchor aria-hidden=true href=#data-augmentation-multiplying-experience-without-playing-more>#</a></h2><p>A very powerful technique to accelerate learning is <strong>data augmentation</strong> (<em>data augmentation</em>).<br>It consists of creating new examples from those we already have, modifying them so that they remain valid but add variety to the training.</p><p>A classic example comes from Chess:<br>imagine an endgame with two rooks trying to checkmate a king.<br>If the enemy king is in the upper-right corner of the board and we achieve mate in three moves, that same sequence is exactly equivalent by symmetry if the king is in any of the other three corners.<br>That is, from a single datum we get four, almost for free.</p><ul><li>In Chess, almost all positions can be duplicated (×2), and most can be quadrupled (×4).</li><li>In Go, thanks to its symmetries and rotations, we can do an ×8 of each data point.</li><li>In Tetris, however, the situation is less favorable: the board has a single possible orientation (gravity always acts downward), so we can only double (×2) the available data.</li></ul><p>This means that Tetris starts with an additional handicap: it not only needs more data, but it can also generate them more slowly and take less advantage of augmentation.</p><h2 id=why-augmentation-is-essential>Why augmentation is essential<a hidden class=anchor aria-hidden=true href=#why-augmentation-is-essential>#</a></h2><p>Data augmentation not only multiplies the amount of available information, it also teaches the system to recognize fundamental patterns.<br>By seeing equivalent situations from different perspectives, the AI learns to generalize better, to understand which features of the board are truly important and which are circumstantial.<br>It’s like teaching a dog to recognize a thief: if it has only seen one dressed in black, it will think all thieves wear black.<br>But if it has seen thieves with jackets, hats, or shirts of different colors, it will learn that what matters is not the color of the clothing, but the fact that they’re taking objects from the owner’s house.</p><h2 id=heuristics-vs-machine-learning-two-different-paths>Heuristics vs. machine learning: two different paths<a hidden class=anchor aria-hidden=true href=#heuristics-vs-machine-learning-two-different-paths>#</a></h2><p>Before the era of machine learning, as in Chess, surprising results were achieved in Tetris using heuristic methods.<br>The idea was simple: simulate all possible positions after placing one or several pieces and evaluate each board with a mathematical function designed by humans.<br>For example, a typical heuristic penalizes holes, rewards complete lines, and seeks to minimize the average height of the columns.<br>The program generates hundreds or thousands of possible boards and chooses the move that produces the highest value according to that function.</p><p>The problem is that a heuristic doesn’t really “think.”<br>It only executes a fixed formula, written by a developer.<br>All the “intelligence” resides in the evaluation function, not in the system.<br>This means the program will never learn anything new: if the creator doesn’t know the best strategy, the AI won’t discover it either.</p><p>Machine learning, by contrast, represents a qualitative leap.<br>It allows us to tackle problems in which the optimal strategy is unknown, because the system itself will discover it during training.<br>Through thousands of games, trial and error, and value adjustments, the AI learns by itself which patterns lead to success, even in environments as chaotic and unpredictable as Tetris.<br>It doesn’t need us to tell it which features make a position good: it eventually infers them.<br>That ability to generate knowledge without direct human intervention is what makes machine learning—and especially reinforcement learning—a revolutionary tool.</p><p>Moreover, the heuristic approach cannot plan strategies several moves deep.<br>This is because the number of configurations grows exponentially with each added piece (in Tetris, on the order of $162^{n}$).<br>If the algorithm tried to foresee the result of, say, five future pieces, it would need to explore $162^{5}≈10^{11}$ — an impossible task in real time.<br>Therefore, heuristics live in the present: they can only optimize the immediate move.<br>Machine learning, by contrast, can anticipate the future by incorporating the expected long-term reward into its training process.<br>This allows it to develop sustainable strategies that not only seek immediate gain (such as clearing a line), but also keep the board stable and adaptable for the upcoming pieces.</p><h2 id=closing>Closing<a hidden class=anchor aria-hidden=true href=#closing>#</a></h2><p>Now that we’ve established the main challenges a machine learning agent faces in Tetris, in the next chapters we’ll explore step by step how my system evolved until reaching the coveted goal:<br>an AI capable of playing Tetris at the highest possible level.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://joan-projects.github.io/en/posts/tetris2/><span class=title>« Prev</span><br><span>Reinforcement Learning in Tetris – Part 2</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on x" href="https://x.com/intent/tweet/?text=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f&amp;title=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;summary=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;source=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f&title=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on whatsapp" href="https://api.whatsapp.com/send?text=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201%20-%20https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on telegram" href="https://telegram.me/share/url?text=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&u=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://joan-projects.github.io/en/>Joan Projects</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>window.addEventListener("load",function(){if(window.renderMathInElement){const e=document.querySelector("article, .post-content, main, body");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})}})</script><script>(function(){if(window.__VC_LIST_READY__)return;window.__VC_LIST_READY__=!0;function e(e){document.querySelectorAll(".vc-entry video").forEach(t=>{t!==e&&t.pause()})}document.addEventListener("click",function(t){const s=t.target.closest(".vc-poster");if(!s)return;t.preventDefault();const i=s.dataset.mp4,a=s.dataset.webm,r=s.dataset.poster,c=s.dataset.controls==="true",u=s.dataset.autoplay==="true",l=s.dataset.loop==="true",d=s.dataset.muted==="true",n=document.createElement("video");if(n.preload="metadata",n.playsInline=!0,r&&(n.poster=r),c&&(n.controls=!0),l&&(n.loop=!0),d&&(n.muted=!0),a){const e=document.createElement("source");e.src=a,e.type="video/webm",n.appendChild(e)}if(i){const e=document.createElement("source");e.src=i,e.type="video/mp4",n.appendChild(e)}const h=s.parentElement;s.replaceWith(n),e(n);const o=n.play();o&&typeof o.catch=="function"&&o.catch(()=>{})},!1)})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>