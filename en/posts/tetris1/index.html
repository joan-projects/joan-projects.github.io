<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Machine Learning in Tetris – Part 1 | Joan Projects</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.
Tetris: the simple game that challenges machine learning
In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.
But there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.
Yes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.">
<meta name="author" content="Joan">
<link rel="canonical" href="https://joan-projects.github.io/en/posts/tetris1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a482bf00fa5c046040ef7973ad5fdf5657890cacb596c93f1382f664ca542074.css" integrity="sha256-pIK/APpcBGBA73lzrV/fVleJDKy1lsk/E4L2ZMpUIHQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://joan-projects.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://joan-projects.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://joan-projects.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://joan-projects.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://joan-projects.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="es" href="https://joan-projects.github.io/posts/tetris1/">
<link rel="alternate" hreflang="en" href="https://joan-projects.github.io/en/posts/tetris1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" crossorigin="anonymous">
	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" crossorigin="anonymous"></script>
	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
	<style>
	   
	  .list .post-entry { 
		display: flex;             
		flex-direction: column;    
	  }
	  .list .post-entry .entry-header { order: 0; }    
	  .list .post-entry .entry-cover  { 
		order: 1;                  
		margin-top: .5rem; 
	  }
	</style>
<meta property="og:url" content="https://joan-projects.github.io/en/posts/tetris1/">
  <meta property="og:site_name" content="Joan Projects">
  <meta property="og:title" content="Machine Learning in Tetris – Part 1">
  <meta property="og:description" content="Introduction Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.
Tetris: the simple game that challenges machine learning In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.
But there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.
Yes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-22T00:00:00+00:00">
    <meta property="og:image" content="https://joan-projects.github.io/img/Tetris1.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://joan-projects.github.io/img/Tetris1.png">
<meta name="twitter:title" content="Machine Learning in Tetris – Part 1">
<meta name="twitter:description" content="Introduction
Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.
Tetris: the simple game that challenges machine learning
In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.
But there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.
Yes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://joan-projects.github.io/en/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Machine Learning in Tetris – Part 1",
      "item": "https://joan-projects.github.io/en/posts/tetris1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Machine Learning in Tetris – Part 1",
  "name": "Machine Learning in Tetris – Part 1",
  "description": "Introduction Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.\nThroughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.\nTetris: the simple game that challenges machine learning In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.\nBut there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.\nYes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.\n",
  "keywords": [
    
  ],
  "articleBody": "Introduction Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.\nThroughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.\nTetris: the simple game that challenges machine learning In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.\nBut there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.\nYes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.\nLet’s see why.\nThe overwhelming number of possibilities To understand the magnitude of the challenge, it’s enough to compare the number of possible situations a player can encounter in each game.\nIn Chess, the number of possible board configurations is around 4,82 × 10⁴⁴. In Go, it skyrockets to 2,08 × 10¹⁷⁰.\nIn comparison, Tetris plays in a different league.\nA Tetris board of 20 rows by 10 columns already offers $2^{20\\times10}$ possible configurations, but that’s only the beginning.\nIf we add the random piece bag factor, and limit ourselves to a short game of about 200 pieces (approximately one minute of play), the total number of possibilities scales to the inconceivable:\n(1,37 × 10³⁷¹) × 2²⁰⁰ ≈ 2,2 × 10⁴³¹\nIn other words, Tetris’s state space is astronomically larger than that of any other classic game.\nTetris is not deterministic: the chaos of randomness And yet, the size of the state space is not the biggest problem.\nThe real difficulty lies in the fact that Tetris is not a deterministic game.\nIn Chess or Go, a specific move always produces the same result. The rules are fixed and the consequences, predictable.\nIn Tetris, however, a decision can be excellent or disastrous depending on which piece comes next.\nThe context is constantly changing and the environment is highly unstable.\nThis means that the evaluation of a position cannot depend only on the current board: the immediate future—unpredictable by nature—changes the value of every action.\nAs a consequence, the well-known MCTS (Monte Carlo Tree Search) method, which was the cornerstone of AlphaZero and AlphaGo, proves ineffective for Tetris. Its structure depends on deterministic scenarios with repeatable outcomes, the exact opposite of Tetris chaos.\nReward, number of actions, and planning The contrast widens when we look at the decision dynamics:\nIn Chess, a player makes about 40 decisions per game. In Go, around 100. In Tetris, by contrast, the number of actions is potentially infinite. And whereas in Chess or Go the final reward is clear—win, draw, or loss—in Tetris the reward can be defined as a score… but with no known upper bound.\nThis makes the reward function an AI must learn enormously complicated.\nAn AI can have a good run, but how does it know if its score is “good” or “excellent”?\nThis lack of reference complicates the design of the reward function, a crucial element in reinforcement learning.\nThe data-gathering problem For an artificial intelligence to truly learn to play Tetris, it’s not enough to show it a few games: it has to play thousands, millions, even hundreds of millions of times.\nOnly then can it begin to grasp the game’s patterns and, little by little, discover for itself the most effective strategies.\nFortunately, computers play infinitely faster than humans. While a person can play a few games in an hour, an AI agent can play millions in the same time, learning from every mistake and every success.\nThe key in this process is to record what happens at every moment: what situation the agent sees, what action it takes, and what result it gets.\nEach of these records is called a transition.\nEach transition is, in essence, a small lesson that the AI stores in its memory to learn how to act the next time it faces something similar.\nTetris versus Chess and Go In Chess, a transition is very quick to compute: in most cases we only need to check whether the move is legal or not; by contrast, in Tetris, not only that, but it is necessary to compute how that move affects the board and build the new piece bag.\nTo be more precise, performing a transition in Chess can be hundreds of times faster than in Tetris.\nTherefore, the total time to obtain training data is much greater.\nHowever, compared to Go, the time per transition is similar.\nNevertheless—and here’s the catch—simulation speed isn’t everything.\nAlthough computers seem capable of playing at unimaginable speeds, the resources of a home computer are not enough to train a complex AI of this kind.\nThe problem is not just computational power, but the time it takes to obtain truly useful data.\nWhy data collection time matters In games like Chess or Go, consequences usually become apparent quite quickly, normally in the next transition. In contrast, in Tetris, when playing at an advanced level, they may become noticeable even more than ten transitions later. That’s why a bad move can look good for several iterations until its consequences show up later.\nThis forces the agent to maintain a “sense of the future”: a memory that links past decisions with delayed outcomes. But how to do this in Tetris? We don’t know which pieces are going to arrive, so what the player sees at any given moment is not easily evaluable.\nAnd if we also want the system to simulate different alternatives before deciding—like the MCTS (Monte Carlo Tree Search) algorithm does—compute time grows exponentially.\nYou can imagine it like a chess player mentally analyzing three, four, or five moves ahead before moving a piece.\nNow imagine they have to do it millions of times per second: that’s the difficulty.\nData augmentation: multiplying experience without playing more A very powerful technique to accelerate learning is data augmentation (data augmentation).\nIt consists of creating new examples from those we already have, modifying them so that they remain valid but add variety to the training.\nA classic example comes from Chess:\nimagine an endgame with two rooks trying to checkmate a king.\nIf the enemy king is in the upper-right corner of the board and we achieve mate in three moves, that same sequence is exactly equivalent by symmetry if the king is in any of the other three corners.\nThat is, from a single datum we get four, almost for free.\nIn Chess, almost all positions can be duplicated (×2), and most can be quadrupled (×4). In Go, thanks to its symmetries and rotations, we can do an ×8 of each data point. In Tetris, however, the situation is less favorable: the board has a single possible orientation (gravity always acts downward), so we can only double (×2) the available data. This means that Tetris starts with an additional handicap: it not only needs more data, but it can also generate them more slowly and take less advantage of augmentation.\nWhy augmentation is essential Data augmentation not only multiplies the amount of available information, it also teaches the system to recognize fundamental patterns.\nBy seeing equivalent situations from different perspectives, the AI learns to generalize better, to understand which features of the board are truly important and which are circumstantial.\nIt’s like teaching a dog to recognize a thief: if it has only seen one dressed in black, it will think all thieves wear black.\nBut if it has seen thieves with jackets, hats, or shirts of different colors, it will learn that what matters is not the color of the clothing, but the fact that they’re taking objects from the owner’s house.\nHeuristics vs. machine learning: two different paths Before the era of machine learning, as in Chess, surprising results were achieved in Tetris using heuristic methods.\nThe idea was simple: simulate all possible positions after placing one or several pieces and evaluate each board with a mathematical function designed by humans.\nFor example, a typical heuristic penalizes holes, rewards complete lines, and seeks to minimize the average height of the columns.\nThe program generates hundreds or thousands of possible boards and chooses the move that produces the highest value according to that function.\nThe problem is that a heuristic doesn’t really “think.”\nIt only executes a fixed formula, written by a developer.\nAll the “intelligence” resides in the evaluation function, not in the system.\nThis means the program will never learn anything new: if the creator doesn’t know the best strategy, the AI won’t discover it either.\nMachine learning, by contrast, represents a qualitative leap.\nIt allows us to tackle problems in which the optimal strategy is unknown, because the system itself will discover it during training.\nThrough thousands of games, trial and error, and value adjustments, the AI learns by itself which patterns lead to success, even in environments as chaotic and unpredictable as Tetris.\nIt doesn’t need us to tell it which features make a position good: it eventually infers them.\nThat ability to generate knowledge without direct human intervention is what makes machine learning—and especially reinforcement learning—a revolutionary tool.\nMoreover, the heuristic approach cannot plan strategies several moves deep.\nThis is because the number of configurations grows exponentially with each added piece (in Tetris, on the order of $162^{n}$).\nIf the algorithm tried to foresee the result of, say, five future pieces, it would need to explore $162^{5}≈10^{11}$ — an impossible task in real time.\nTherefore, heuristics live in the present: they can only optimize the immediate move.\nMachine learning, by contrast, can anticipate the future by incorporating the expected long-term reward into its training process.\nThis allows it to develop sustainable strategies that not only seek immediate gain (such as clearing a line), but also keep the board stable and adaptable for the upcoming pieces.\nClosing Now that we’ve established the main challenges a machine learning agent faces in Tetris, in the next chapters we’ll explore step by step how my system evolved until reaching the coveted goal:\nan AI capable of playing Tetris at the highest possible level.\n",
  "wordCount" : "1752",
  "inLanguage": "en",
  "image":"https://joan-projects.github.io/img/Tetris1.png","datePublished": "2025-10-22T00:00:00Z",
  "dateModified": "2025-10-22T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Joan"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://joan-projects.github.io/en/posts/tetris1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Joan Projects",
    "logo": {
      "@type": "ImageObject",
      "url": "https://joan-projects.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://joan-projects.github.io/en/" accesskey="h" title="Joan Projects (Alt + H)">Joan Projects</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                    <ul class="lang-switch"><li>|</li>
                        <li>
                            <a href="https://joan-projects.github.io/" title="Español"
                                aria-label="Español">Es</a>
                        </li>
                    </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://joan-projects.github.io/en/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://joan-projects.github.io/en/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://joan-projects.github.io/en/contact/" title="Contact">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Machine Learning in Tetris – Part 1
    </h1>
    <div class="post-meta"><span title='2025-10-22 00:00:00 +0000 UTC'>October 22, 2025</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>Joan</span>&nbsp;|&nbsp;<span>Translations:</span>
<ul class="i18n_list">
    <li>
        <a href="https://joan-projects.github.io/posts/tetris1/">Es</a>
    </li>
</ul>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="https://joan-projects.github.io/img/Tetris1.png" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#tetris-the-simple-game-that-challenges-machine-learning" aria-label="Tetris: the simple game that challenges machine learning">Tetris: the simple game that challenges machine learning</a></li>
                <li>
                    <a href="#the-overwhelming-number-of-possibilities" aria-label="The overwhelming number of possibilities">The overwhelming number of possibilities</a></li>
                <li>
                    <a href="#tetris-is-not-deterministic-the-chaos-of-randomness" aria-label="Tetris is not deterministic: the chaos of randomness">Tetris is not deterministic: the chaos of randomness</a></li>
                <li>
                    <a href="#reward-number-of-actions-and-planning" aria-label="Reward, number of actions, and planning">Reward, number of actions, and planning</a></li>
                <li>
                    <a href="#the-data-gathering-problem" aria-label="The data-gathering problem">The data-gathering problem</a></li>
                <li>
                    <a href="#tetris-versus-chess-and-go" aria-label="Tetris versus Chess and Go">Tetris versus Chess and Go</a></li>
                <li>
                    <a href="#why-data-collection-time-matters" aria-label="Why data collection time matters">Why data collection time matters</a></li>
                <li>
                    <a href="#data-augmentation-multiplying-experience-without-playing-more" aria-label="Data augmentation: multiplying experience without playing more">Data augmentation: multiplying experience without playing more</a></li>
                <li>
                    <a href="#why-augmentation-is-essential" aria-label="Why augmentation is essential">Why augmentation is essential</a></li>
                <li>
                    <a href="#heuristics-vs-machine-learning-two-different-paths" aria-label="Heuristics vs. machine learning: two different paths">Heuristics vs. machine learning: two different paths</a></li>
                <li>
                    <a href="#closing" aria-label="Closing">Closing</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Here begins a series of articles where I’ll tell you about my journey to develop an artificial intelligence capable of playing Tetris at the highest level, in real time, within the tetr.io environment — the same one where professional players compete today.<br>
Throughout this series I’ll share and describe the milestones achieved up to the publication of the final repository with the fully trained AI.</p>
<h2 id="tetris-the-simple-game-that-challenges-machine-learning">Tetris: the simple game that challenges machine learning<a hidden class="anchor" aria-hidden="true" href="#tetris-the-simple-game-that-challenges-machine-learning">#</a></h2>
<p>In the history of machine learning, there are canonical, legendary moments: AlphaGo and AlphaZero achieved superhuman performance in Go and Chess, proving that an artificial intelligence can learn by itself—simply by playing against itself, without any human help—until it surpasses the world’s best players in games where strategy is everything.<br>
But there is one game that, although it seems simpler, poses an even greater challenge for an AI: Tetris.<br>
Yes, that game of relentlessly falling pieces can be a true nightmare for machine learning systems when trying to reach a level comparable to—or higher than—an experienced player’s, and even more so a professional’s.</p>
<p>Let’s see why.</p>
<h2 id="the-overwhelming-number-of-possibilities">The overwhelming number of possibilities<a hidden class="anchor" aria-hidden="true" href="#the-overwhelming-number-of-possibilities">#</a></h2>
<p>To understand the magnitude of the challenge, it’s enough to compare the number of possible situations a player can encounter in each game.<br>
In Chess, the number of possible board configurations is around 4,82 × 10⁴⁴. In Go, it skyrockets to 2,08 × 10¹⁷⁰.<br>
In comparison, Tetris plays in a different league.<br>
A Tetris board of 20 rows by 10 columns already offers $2^{20\times10}$
possible configurations, but that’s only the beginning.<br>
If we add the random piece bag factor, and limit ourselves to a short game of about 200 pieces (approximately one minute of play), the total number of possibilities scales to the inconceivable:</p>
<p>(1,37 × 10³⁷¹) × 2²⁰⁰ ≈ 2,2 × 10⁴³¹</p>
<p>In other words, Tetris’s state space is astronomically larger than that of any other classic game.</p>
<h2 id="tetris-is-not-deterministic-the-chaos-of-randomness">Tetris is not deterministic: the chaos of randomness<a hidden class="anchor" aria-hidden="true" href="#tetris-is-not-deterministic-the-chaos-of-randomness">#</a></h2>
<p>And yet, the size of the state space is not the biggest problem.<br>
The real difficulty lies in the fact that Tetris is not a deterministic game.<br>
In Chess or Go, a specific move always produces the same result. The rules are fixed and the consequences, predictable.<br>
In Tetris, however, a decision can be excellent or disastrous depending on which piece comes next.<br>
The context is constantly changing and the environment is highly unstable.<br>
This means that the evaluation of a position cannot depend only on the current board: the immediate future—unpredictable by nature—changes the value of every action.<br>
As a consequence, the well-known MCTS (Monte Carlo Tree Search) method, which was the cornerstone of AlphaZero and AlphaGo, proves ineffective for Tetris. Its structure depends on deterministic scenarios with repeatable outcomes, the exact opposite of Tetris chaos.</p>
<h2 id="reward-number-of-actions-and-planning">Reward, number of actions, and planning<a hidden class="anchor" aria-hidden="true" href="#reward-number-of-actions-and-planning">#</a></h2>
<p>The contrast widens when we look at the decision dynamics:</p>
<ul>
<li>In Chess, a player makes about 40 decisions per game.</li>
<li>In Go, around 100.</li>
<li>In Tetris, by contrast, the number of actions is potentially infinite.</li>
</ul>
<p>And whereas in Chess or Go the final reward is clear—win, draw, or loss—in Tetris the reward can be defined as a score… but with no known upper bound.<br>
This makes the reward function an AI must learn enormously complicated.<br>
An AI can have a good run, but how does it know if its score is “good” or “excellent”?<br>
This lack of reference complicates the design of the reward function, a crucial element in reinforcement learning.</p>
<h2 id="the-data-gathering-problem">The data-gathering problem<a hidden class="anchor" aria-hidden="true" href="#the-data-gathering-problem">#</a></h2>
<p>For an artificial intelligence to truly learn to play Tetris, it’s not enough to show it a few games: it has to play thousands, millions, even hundreds of millions of times.<br>
Only then can it begin to grasp the game’s patterns and, little by little, discover for itself the most effective strategies.<br>
Fortunately, computers play infinitely faster than humans. While a person can play a few games in an hour, an AI agent can play millions in the same time, learning from every mistake and every success.<br>
The key in this process is to record what happens at every moment: what situation the agent sees, what action it takes, and what result it gets.<br>
Each of these records is called a <strong>transition</strong>.<br>
Each transition is, in essence, a small lesson that the AI stores in its memory to learn how to act the next time it faces something similar.</p>
<h2 id="tetris-versus-chess-and-go">Tetris versus Chess and Go<a hidden class="anchor" aria-hidden="true" href="#tetris-versus-chess-and-go">#</a></h2>
<p>In Chess, a transition is very quick to compute: in most cases we only need to check whether the move is legal or not; by contrast, in Tetris, not only that, but it is necessary to compute how that move affects the board and build the new piece bag.<br>
To be more precise, performing a transition in Chess can be hundreds of times faster than in Tetris.<br>
Therefore, the total time to obtain training data is much greater.<br>
However, compared to Go, the time per transition is similar.<br>
Nevertheless—and here’s the catch—simulation speed isn’t everything.<br>
Although computers seem capable of playing at unimaginable speeds, the resources of a home computer are not enough to train a complex AI of this kind.<br>
The problem is not just computational power, but the time it takes to obtain truly useful data.</p>
<h2 id="why-data-collection-time-matters">Why data collection time matters<a hidden class="anchor" aria-hidden="true" href="#why-data-collection-time-matters">#</a></h2>
<p>In games like Chess or Go, consequences usually become apparent quite quickly, normally in the next transition. In contrast, in Tetris, when playing at an advanced level, they may become noticeable even more than ten transitions later.
That’s why a bad move can look good for several iterations until its consequences show up later.<br>
This forces the agent to maintain a “sense of the future”: a memory that links past decisions with delayed outcomes.
But how to do this in Tetris? We don’t know which pieces are going to arrive, so what the player sees at any given moment is not easily evaluable.<br>
And if we also want the system to simulate different alternatives before deciding—like the MCTS (Monte Carlo Tree Search) algorithm does—compute time grows exponentially.<br>
You can imagine it like a chess player mentally analyzing three, four, or five moves ahead before moving a piece.<br>
Now imagine they have to do it millions of times per second: that’s the difficulty.</p>
<h2 id="data-augmentation-multiplying-experience-without-playing-more">Data augmentation: multiplying experience without playing more<a hidden class="anchor" aria-hidden="true" href="#data-augmentation-multiplying-experience-without-playing-more">#</a></h2>
<p>A very powerful technique to accelerate learning is <strong>data augmentation</strong> (<em>data augmentation</em>).<br>
It consists of creating new examples from those we already have, modifying them so that they remain valid but add variety to the training.</p>
<p>A classic example comes from Chess:<br>
imagine an endgame with two rooks trying to checkmate a king.<br>
If the enemy king is in the upper-right corner of the board and we achieve mate in three moves, that same sequence is exactly equivalent by symmetry if the king is in any of the other three corners.<br>
That is, from a single datum we get four, almost for free.</p>
<ul>
<li>In Chess, almost all positions can be duplicated (×2), and most can be quadrupled (×4).</li>
<li>In Go, thanks to its symmetries and rotations, we can do an ×8 of each data point.</li>
<li>In Tetris, however, the situation is less favorable: the board has a single possible orientation (gravity always acts downward), so we can only double (×2) the available data.</li>
</ul>
<p>This means that Tetris starts with an additional handicap: it not only needs more data, but it can also generate them more slowly and take less advantage of augmentation.</p>
<h2 id="why-augmentation-is-essential">Why augmentation is essential<a hidden class="anchor" aria-hidden="true" href="#why-augmentation-is-essential">#</a></h2>
<p>Data augmentation not only multiplies the amount of available information, it also teaches the system to recognize fundamental patterns.<br>
By seeing equivalent situations from different perspectives, the AI learns to generalize better, to understand which features of the board are truly important and which are circumstantial.<br>
It’s like teaching a dog to recognize a thief: if it has only seen one dressed in black, it will think all thieves wear black.<br>
But if it has seen thieves with jackets, hats, or shirts of different colors, it will learn that what matters is not the color of the clothing, but the fact that they’re taking objects from the owner’s house.</p>
<h2 id="heuristics-vs-machine-learning-two-different-paths">Heuristics vs. machine learning: two different paths<a hidden class="anchor" aria-hidden="true" href="#heuristics-vs-machine-learning-two-different-paths">#</a></h2>
<p>Before the era of machine learning, as in Chess, surprising results were achieved in Tetris using heuristic methods.<br>
The idea was simple: simulate all possible positions after placing one or several pieces and evaluate each board with a mathematical function designed by humans.<br>
For example, a typical heuristic penalizes holes, rewards complete lines, and seeks to minimize the average height of the columns.<br>
The program generates hundreds or thousands of possible boards and chooses the move that produces the highest value according to that function.</p>
<p>The problem is that a heuristic doesn’t really “think.”<br>
It only executes a fixed formula, written by a developer.<br>
All the “intelligence” resides in the evaluation function, not in the system.<br>
This means the program will never learn anything new: if the creator doesn’t know the best strategy, the AI won’t discover it either.</p>
<p>Machine learning, by contrast, represents a qualitative leap.<br>
It allows us to tackle problems in which the optimal strategy is unknown, because the system itself will discover it during training.<br>
Through thousands of games, trial and error, and value adjustments, the AI learns by itself which patterns lead to success, even in environments as chaotic and unpredictable as Tetris.<br>
It doesn’t need us to tell it which features make a position good: it eventually infers them.<br>
That ability to generate knowledge without direct human intervention is what makes machine learning—and especially reinforcement learning—a revolutionary tool.</p>
<p>Moreover, the heuristic approach cannot plan strategies several moves deep.<br>
This is because the number of configurations grows exponentially with each added piece (in Tetris, on the order of $162^{n}$).<br>
If the algorithm tried to foresee the result of, say, five future pieces, it would need to explore $162^{5}≈10^{11}$ — an impossible task in real time.<br>
Therefore, heuristics live in the present: they can only optimize the immediate move.<br>
Machine learning, by contrast, can anticipate the future by incorporating the expected long-term reward into its training process.<br>
This allows it to develop sustainable strategies that not only seek immediate gain (such as clearing a line), but also keep the board stable and adaptable for the upcoming pieces.</p>
<h2 id="closing">Closing<a hidden class="anchor" aria-hidden="true" href="#closing">#</a></h2>
<p>Now that we’ve established the main challenges a machine learning agent faces in Tetris, in the next chapters we’ll explore step by step how my system evolved until reaching the coveted goal:<br>
an AI capable of playing Tetris at the highest possible level.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on x"
            href="https://x.com/intent/tweet/?text=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f&amp;title=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;summary=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;source=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f&title=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on whatsapp"
            href="https://api.whatsapp.com/send?text=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201%20-%20https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on telegram"
            href="https://telegram.me/share/url?text=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&amp;url=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Machine Learning in Tetris – Part 1 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20in%20Tetris%20%e2%80%93%20Part%201&u=https%3a%2f%2fjoan-projects.github.io%2fen%2fposts%2ftetris1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://joan-projects.github.io/en/">Joan Projects</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>
<script>
  window.addEventListener("load", function () {
    if (window.renderMathInElement) {
      const root = document.querySelector("article, .post-content, main, body");
      renderMathInElement(root, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\(", right: "\\)", display: false},
          {left: "\\[", right: "\\]", display: true}
        ],
        throwOnError: false
        
        
        
      });
    }
  });
</script>



<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
