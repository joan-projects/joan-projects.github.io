<!doctype html><html lang=es dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Aprendizaje por refuerzo en Tetris – Parte 2 | Joan — Projects —</title><meta name=keywords content><meta name=description content="Introducción
En esta segunda entrega te voy a contar cómo abordé el desafío del entrenamiento de un agente de Reinforcement Learning aplicado a Tetris, teniendo en cuenta las dificultades que mencioné en el artículo anterior.
El objetivo principal es alcanzar un nivel sobrehumano, pero no en el sentido de que el agente juegue más rápido o con mayor precisión que una persona —eso, al fin y al cabo, es lo esperable de una máquina—, sino porque sea mejor estratega. De hecho, nuestro agente jugará a una velocidad similar a la de un jugador bueno (menos de 2.5 piezas por segundo), e incluso le obligaré a cometer errores de vez en cuando (5% de probabilidad de cometer error).
Voy a contarte las primeras aproximaciones que dieron buenos resultados y que sirvieron de base para las siguientes, que omitiré aquí para no hacer el artículo demasiado extenso ni excesivamente técnico o matemático.
Imagino que lo primero que habrás hecho ha sido darle al play y comprobado el sorprendente nivel alcanzado. Es sorprendente y todo un logro para el aprendizaje por refuerzo. He conseguido que una IA aprenda estrategias efectivas por sí sola, sin recibir ningún tipo de consejo; como ves, planifica, es capaz de prever situaciones futuras sin conocer las próximas piezas, realiza combos y no se deja llevar por la recompensa inmediata.
Así que, si el video ya te dejó con curiosidad, te invito a seguir leyendo. ¡Vamos allá!"><meta name=author content="Joan"><link rel=canonical href=https://joan-projects.github.io/posts/tetris2/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://joan-projects.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://joan-projects.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://joan-projects.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://joan-projects.github.io/apple-touch-icon.png><link rel=mask-icon href=https://joan-projects.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=es href=https://joan-projects.github.io/posts/tetris2/><link rel=alternate hreflang=en href=https://joan-projects.github.io/en/posts/tetris2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js crossorigin=anonymous></script><style>.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><style>.vc-entry{position:relative}.vc-poster{position:relative;display:block;width:100%;padding:0;border:0;background:0 0;cursor:pointer}.vc-poster .entry-cover-image{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-entry video{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-play{position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);font-size:3rem;line-height:1;opacity:.9;background:rgba(0,0,0,.35);color:#fff;padding:.25em .5em;border-radius:.5em}@media(max-width:640px){.vc-play{font-size:2.2rem}}.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><meta property="og:url" content="https://joan-projects.github.io/posts/tetris2/"><meta property="og:site_name" content="Joan — Projects —"><meta property="og:title" content="Aprendizaje por refuerzo en Tetris – Parte 2"><meta property="og:description" content="Introducción En esta segunda entrega te voy a contar cómo abordé el desafío del entrenamiento de un agente de Reinforcement Learning aplicado a Tetris, teniendo en cuenta las dificultades que mencioné en el artículo anterior.
El objetivo principal es alcanzar un nivel sobrehumano, pero no en el sentido de que el agente juegue más rápido o con mayor precisión que una persona —eso, al fin y al cabo, es lo esperable de una máquina—, sino porque sea mejor estratega. De hecho, nuestro agente jugará a una velocidad similar a la de un jugador bueno (menos de 2.5 piezas por segundo), e incluso le obligaré a cometer errores de vez en cuando (5% de probabilidad de cometer error).
Voy a contarte las primeras aproximaciones que dieron buenos resultados y que sirvieron de base para las siguientes, que omitiré aquí para no hacer el artículo demasiado extenso ni excesivamente técnico o matemático.
Imagino que lo primero que habrás hecho ha sido darle al play y comprobado el sorprendente nivel alcanzado. Es sorprendente y todo un logro para el aprendizaje por refuerzo. He conseguido que una IA aprenda estrategias efectivas por sí sola, sin recibir ningún tipo de consejo; como ves, planifica, es capaz de prever situaciones futuras sin conocer las próximas piezas, realiza combos y no se deja llevar por la recompensa inmediata.
Así que, si el video ya te dejó con curiosidad, te invito a seguir leyendo. ¡Vamos allá!"><meta property="og:locale" content="es-es"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-05T00:00:00+00:00"><meta property="og:image" content="https://joan-projects.github.io/img/Tetris2.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://joan-projects.github.io/img/Tetris2.png"><meta name=twitter:title content="Aprendizaje por refuerzo en Tetris – Parte 2"><meta name=twitter:description content="Introducción
En esta segunda entrega te voy a contar cómo abordé el desafío del entrenamiento de un agente de Reinforcement Learning aplicado a Tetris, teniendo en cuenta las dificultades que mencioné en el artículo anterior.
El objetivo principal es alcanzar un nivel sobrehumano, pero no en el sentido de que el agente juegue más rápido o con mayor precisión que una persona —eso, al fin y al cabo, es lo esperable de una máquina—, sino porque sea mejor estratega. De hecho, nuestro agente jugará a una velocidad similar a la de un jugador bueno (menos de 2.5 piezas por segundo), e incluso le obligaré a cometer errores de vez en cuando (5% de probabilidad de cometer error).
Voy a contarte las primeras aproximaciones que dieron buenos resultados y que sirvieron de base para las siguientes, que omitiré aquí para no hacer el artículo demasiado extenso ni excesivamente técnico o matemático.
Imagino que lo primero que habrás hecho ha sido darle al play y comprobado el sorprendente nivel alcanzado. Es sorprendente y todo un logro para el aprendizaje por refuerzo. He conseguido que una IA aprenda estrategias efectivas por sí sola, sin recibir ningún tipo de consejo; como ves, planifica, es capaz de prever situaciones futuras sin conocer las próximas piezas, realiza combos y no se deja llevar por la recompensa inmediata.
Así que, si el video ya te dejó con curiosidad, te invito a seguir leyendo. ¡Vamos allá!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://joan-projects.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Aprendizaje por refuerzo en Tetris – Parte 2","item":"https://joan-projects.github.io/posts/tetris2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Aprendizaje por refuerzo en Tetris – Parte 2","name":"Aprendizaje por refuerzo en Tetris – Parte 2","description":"Introducción En esta segunda entrega te voy a contar cómo abordé el desafío del entrenamiento de un agente de Reinforcement Learning aplicado a Tetris, teniendo en cuenta las dificultades que mencioné en el artículo anterior.\nEl objetivo principal es alcanzar un nivel sobrehumano, pero no en el sentido de que el agente juegue más rápido o con mayor precisión que una persona —eso, al fin y al cabo, es lo esperable de una máquina—, sino porque sea mejor estratega. De hecho, nuestro agente jugará a una velocidad similar a la de un jugador bueno (menos de 2.5 piezas por segundo), e incluso le obligaré a cometer errores de vez en cuando (5% de probabilidad de cometer error).\nVoy a contarte las primeras aproximaciones que dieron buenos resultados y que sirvieron de base para las siguientes, que omitiré aquí para no hacer el artículo demasiado extenso ni excesivamente técnico o matemático.\nImagino que lo primero que habrás hecho ha sido darle al play y comprobado el sorprendente nivel alcanzado. Es sorprendente y todo un logro para el aprendizaje por refuerzo. He conseguido que una IA aprenda estrategias efectivas por sí sola, sin recibir ningún tipo de consejo; como ves, planifica, es capaz de prever situaciones futuras sin conocer las próximas piezas, realiza combos y no se deja llevar por la recompensa inmediata.\nAsí que, si el video ya te dejó con curiosidad, te invito a seguir leyendo. ¡Vamos allá!\n","keywords":[],"articleBody":"Introducción En esta segunda entrega te voy a contar cómo abordé el desafío del entrenamiento de un agente de Reinforcement Learning aplicado a Tetris, teniendo en cuenta las dificultades que mencioné en el artículo anterior.\nEl objetivo principal es alcanzar un nivel sobrehumano, pero no en el sentido de que el agente juegue más rápido o con mayor precisión que una persona —eso, al fin y al cabo, es lo esperable de una máquina—, sino porque sea mejor estratega. De hecho, nuestro agente jugará a una velocidad similar a la de un jugador bueno (menos de 2.5 piezas por segundo), e incluso le obligaré a cometer errores de vez en cuando (5% de probabilidad de cometer error).\nVoy a contarte las primeras aproximaciones que dieron buenos resultados y que sirvieron de base para las siguientes, que omitiré aquí para no hacer el artículo demasiado extenso ni excesivamente técnico o matemático.\nImagino que lo primero que habrás hecho ha sido darle al play y comprobado el sorprendente nivel alcanzado. Es sorprendente y todo un logro para el aprendizaje por refuerzo. He conseguido que una IA aprenda estrategias efectivas por sí sola, sin recibir ningún tipo de consejo; como ves, planifica, es capaz de prever situaciones futuras sin conocer las próximas piezas, realiza combos y no se deja llevar por la recompensa inmediata.\nAsí que, si el video ya te dejó con curiosidad, te invito a seguir leyendo. ¡Vamos allá!\nPrimeros pasos El primer paso —que quizá des por hecho, pero vale la pena mencionar— consiste en implementar un entorno de juego que replique con precisión el comportamiento de una partida real en tetr.io, ya que ese será el escenario final donde quiero que juegue el agente.\nSin embargo, no entrenamos directamente en tetr.io. La razón principal es que nuestro entorno necesita estar totalmente optimizado en términos de tiempo, de modo que se puedan ejecutar partidas a la mayor velocidad posible.\nPara que te hagas una idea, el entorno que desarrollé es capaz de simular partidas cientos de veces más rápido que lo que podríamos lograr jugando directamente en tetr.io.\nModelado de la recompensa Como mencioné en la primera parte, uno de los principales desafíos del Reinforcement Learning aplicado a Tetris es el diseño de la función de recompensa. Las partidas pueden ser potencialmente infinitas, y para entrenar correctamente al agente es fundamental poder determinar qué partida ha sido mejor jugada que otra.\nUna primera idea sencilla para resolverlo sería limitar el número de piezas por partida, por ejemplo, a 100. De este modo, podríamos comparar el rendimiento de las partidas según su puntuación final, haciendo que el agente aprenda a maximizar su puntuación dentro de ese margen.\nAunque esta aproximación inicial es válida, presenta varios problemas:\nCrecimiento de la dificultad.\nEn Tetris, obtener puntuaciones más altas se vuelve exponencialmente más difícil a medida que avanza la partida. Sin embargo, este modelo lineal de recompensa no refleja esa complejidad, y queremos que el agente aprenda precisamente a reconocer y adaptarse a ese aumento de dificultad.\nValor temporal de las puntuaciones.\nPuede ocurrir que un agente consiga una cierta puntuación rápidamente y luego se estanque, mientras que otro alcance la misma puntuación, pero solo al final de la partida. Si ambos obtienen la misma recompensa final, el sistema considerará que han rendido igual, cuando en realidad el primero ha demostrado una estrategia más eficiente en algunos puntos. Por tanto, debemos diseñar la recompensa de forma que premie los picos de puntuación o las puntuaciones altas tempranas.\nEstado final de la partida.\nOtro caso común es que dos partidas terminen con la misma puntuación, pero una de ellas lo haga al borde del colapso, mientras que la otra mantenga el tablero en una situación estable. En ese caso, el modelo básico consideraría que ambas son igual de buenas, cuando la segunda claramente refleja una gestión más sólida. Una forma sencilla de abordar esto es variar el número de piezas por partida: un agente que termina “al borde de la muerte” tras 100 piezas obtendría una puntuación peor cuando se le exija colocar 150, penalizando indirectamente las estrategias inestables.\nIncentivar la curiosidad Cuando una persona se enfrenta a una situación completamente nueva —por ejemplo, un juego que nunca ha probado—, lo primero que hace es experimentar: prueba diferentes acciones, observa los resultados y aprende de ellos. Este proceso de ensayo y error es esencial para entender cómo funciona el entorno.\nNuestro agente debe hacer exactamente lo mismo. No basta con que repita las acciones que le dieron buenos resultados en el pasado; eso lo llevaría rápidamente a un aprendizaje limitado o sesgado. En su lugar, debe mantener una curiosidad activa, explorando también aquellas acciones cuyo efecto desconoce.\nPara conseguir esto, implementé un mecanismo de curiosidad artificial mediante una segunda red neuronal auxiliar, que se ejecuta en paralelo al agente principal. Esta red tiene un objetivo muy concreto: predecir lo que va a ocurrir en el juego cuando el agente realiza una determinada acción.\nPor ejemplo, si el agente decide colocar una pieza en una posición poco habitual, la red neuronal intenta anticipar el resultado: cómo quedará el tablero, cuántas líneas se eliminarán, si se generarán huecos, etc.\nDespués de que la acción se ejecute realmente, comparamos la predicción de la red con el resultado real.\nSi la predicción ha sido muy precisa, significa que esa acción ya es bien conocida por el agente, y por tanto no se necesita incentivar más su exploración. Pero si la predicción falla o difiere mucho de lo ocurrido en la práctica, interpretamos que el agente ha descubierto una situación nueva o poco explorada. En ese caso, se le otorga una recompensa adicional, para motivarlo a seguir investigando ese tipo de acciones. Este mecanismo actúa, en esencia, como una fuente de motivación interna: el agente siente “curiosidad” cuando algo le sorprende, y se siente atraído a seguir probando hasta que logra comprenderlo. Una vez domina esa situación, la curiosidad desaparece y su comportamiento vuelve a centrarse en maximizar la recompensa externa (la puntuación del juego).\nSin embargo, hay un aspecto crucial que conviene destacar.\nSi la red de predicción mejora demasiado y llega a anticipar con precisión todo lo que puede suceder, el agente deja de experimentar sorpresa ante el entorno. En términos prácticos, esto significa que pierde la curiosidad, ya que todas las acciones le resultan previsibles. Como consecuencia, su capacidad para seguir descubriendo estrategias nuevas o más efectivas se reduce drásticamente.\nPara evitar este estancamiento, es necesario introducir cierto “olvido controlado” en la red que genera la curiosidad. Dicho de otro modo, hacemos que la red pierda parte de su conocimiento de vez en cuando, o degradamos ligeramente su capacidad de predicción con el tiempo. Esto puede parecer contraproducente —¿por qué querríamos que una red “olvide”?—, pero tiene un propósito muy importante: reactivar la curiosidad del agente hacia situaciones que ya había explorado antes.\nImagina lo siguiente: al inicio del entrenamiento, el agente todavía no es muy hábil. Cuando se enfrenta a una determinada configuración del tablero y experimenta con una acción nueva, puede que la explore y satisfaga su curiosidad en ese momento, pero sin aprovechar realmente todo su potencial estratégico, porque aún no tiene las habilidades suficientes para hacerlo.\nCon el tiempo, el agente mejora, desarrolla estrategias más avanzadas y aprende a interpretar el entorno de manera más sofisticada. En ese punto, esas mismas situaciones que antes ya conocía pueden tener nuevas posibilidades que antes no era capaz de reconocer.\nSi la red de curiosidad conserva intacto su conocimiento anterior, no volverá a considerar esas situaciones como “interesantes”, y el agente nunca regresará a ellas para redescubrirlas con su nueva perspectiva. Por eso es importante que la red “olvide” parcialmente lo aprendido: al hacerlo, vuelve a generar curiosidad ante esos patrones antiguos, permitiendo que el agente los reexplore con un nivel de competencia superior.\nAgentes en jerarquía En muchos videojuegos, las habilidades de un jugador pueden analizarse en dos niveles complementarios: el macrogame y el microgame.\nEl macrogame hace referencia a la dimensión estratégica del juego: la capacidad de planificar, anticiparse a los eventos y tomar decisiones que determinen el rumbo general de la partida. Es la parte que responde al “qué hacer” y “por qué hacerlo”.\nPor su parte, el microgame describe la dimensión técnica o de ejecución: la precisión, la rapidez y la coordinación necesarias para llevar a cabo de forma efectiva las decisiones estratégicas previamente tomadas. Es la parte que responde al “cómo hacerlo”.\nSi trasladamos estos conceptos al contexto de Tetris, el macrogame corresponde a la planificación global: decidir dónde colocar cada pieza considerando el estado del tablero y, cuando es posible, anticipando las piezas futuras. En cambio, el microgame se refleja en la ejecución exacta de esa decisión: la rapidez con la que se rota la pieza, la secuencia de teclas necesarias y la precisión con que finalmente se coloca en su posición ideal.\nAmbos niveles son complementarios: el jugador más competente es aquel que logra combinar una visión estratégica sólida con una ejecución precisa y eficiente. Sin embargo, son dos habilidades muy diferentes entre sí, tanto en la forma en que se adquieren como en los procesos cognitivos que las sustentan.\nCon esta distinción en mente, opté por diseñar dos agentes de inteligencia artificial jerarquizados, cada uno especializado en uno de los niveles:\nEl agente estratega (macro): encargado de decidir dónde debe colocarse la siguiente pieza en función del estado del tablero. El agente ejecutor (micro): responsable de traducir esa decisión en acciones concretas, es decir, calcular qué teclas debe presionar y en qué orden para que la pieza llegue a la posición deseada. Esta estructura jerárquica presenta múltiples ventajas:\nEntrenamiento más eficiente.\nCada agente se entrena para cumplir una tarea muy específica, lo que simplifica el proceso de aprendizaje y acelera la convergencia.\nSeparación de errores.\nAl dividir las responsabilidades, no castigamos una habilidad por los fallos de la otra. Por ejemplo, si el agente estratega toma la mejor decisión posible pero el ejecutor comete un error al colocar la pieza, no penalizamos al estratega por un fallo que no le corresponde. Esto evita confundir la retroalimentación y permite que cada agente aprenda de forma más estable.\nDiferentes Arquitecturas neuronales.\nUna de las principales ventajas de dividir el sistema en dos agentes jerarquizados es que podemos diseñar arquitecturas de red neuronal especializadas para cada tipo de tarea. Aunque ambos agentes trabajan con información espacial —por lo que es natural utilizar redes convolucionales (CNNs) para procesar el estado del tablero—, los objetivos y dinámicas de aprendizaje son tan distintos que no tiene sentido utilizar la misma arquitectura ni los mismos hiperparámetros en ambos casos.\nEl agente estratega, encargado del macrogame, necesita una red con mayor profundidad y capacidad de abstracción, capaz de identificar patrones globales y relaciones de largo alcance dentro del tablero. En su caso, conviene incorporar capas de atención o bloques residuales que faciliten la propagación del gradiente y la comprensión de dependencias espaciales complejas. Además, suele beneficiarse de técnicas de regularización moderadas, como dropout o weight decay suaves, para evitar el sobreajuste pero sin limitar su capacidad de exploración. En cuanto a los hiperparámetros, un tamaño de lote mayor (batch size) y tasas de aprendizaje más pequeñas suelen ayudar a estabilizar el proceso, ya que su entrenamiento es más sensible a variaciones pequeñas en la recompensa.\nPor otro lado, el agente ejecutor, centrado en el microgame, requiere una arquitectura más ligera y reactiva, optimizada para la rapidez y la precisión en la toma de decisiones. En este caso, se priorizan redes más superficiales, con menos capas convolucionales pero más densamente conectadas, lo que permite respuestas inmediatas ante cambios del entorno. Es habitual aplicar reguladores más estrictos, como batch normalization y dropout más agresivos, para forzar la generalización y evitar que memorice secuencias de movimientos concretas. En cuanto a los hiperparámetros, convienen tasas de aprendizaje más altas y entrenamientos con actualizaciones frecuentes, favoreciendo una adaptación ágil a los patrones del juego.\nAumentación de datos resuelta mediante atención Como mencioné en la primera parte, una forma eficaz de ayudar al agente a reconocer patrones de juego consiste en exponerlo a situaciones diferentes que comparten la misma solución estratégica. En Tetris, por ejemplo, dos tableros que parecen distintos pueden requerir exactamente la misma jugada si su estructura es simétrica o equivalente en su parte superior.\nUna forma sencilla de lograr esto es mediante transformaciones del entorno, como aplicar simetrías horizontales tanto al tablero como a las piezas. Esto duplica, por así decirlo, los ejemplos disponibles, ayudando al agente a generalizar sus estrategias. Sin embargo, descubrí que existía una alternativa aún más poderosa y eficiente: introducir un sistema de atención en el propio tablero.\nEste sistema de atención actúa como un filtro visual que limita la cantidad de información que el agente estratega puede procesar simultáneamente. Concretamente, el agente solo puede centrarse en cuatro líneas del tablero a la vez, que en la mayoría de los casos acaban siendo —por pura optimización del entrenamiento— las cuatro líneas superiores, donde se toman las decisiones más críticas.\nEsta restricción tiene un efecto muy interesante: el agente aprende a abstraer el contexto y a reconocer patrones equivalentes en diferentes alturas del tablero. Es decir, interioriza que la forma de resolver una situación no depende de su posición absoluta, sino de la configuración local de las piezas. En la práctica, esto funciona como una forma de aumentación de datos implícita, ya que el agente aprende que muchos escenarios son esencialmente el mismo problema trasladado verticalmente.\nGracias a este enfoque, el proceso de entrenamiento inicial se vuelve mucho más eficiente, porque el agente necesita ver menos ejemplos distintos para comprender principios generales de juego. Dicho de otro modo, aprende a “pensar por analogía”: una situación que antes consideraba única pasa a verse como una variación de algo que ya domina.\nSin embargo, este sistema de atención debe evolucionar junto con el agente. En las fases tempranas, limitar el foco a solo cuatro líneas facilita el aprendizaje al reducir la complejidad perceptiva. Pero a medida que el agente se vuelve más competente, esa restricción se convierte en una limitación. Para alcanzar un rendimiento de nivel profesional, el agente necesita ampliar progresivamente su campo de atención hasta abarcar la totalidad del tablero, integrando la información de las líneas inferiores para realizar una planificación completa.\nLuchando contra un entorno aleatorio Aprender en un entorno tan caótico como una partida de Tetris es, literalmente, un reto épico. Como mencionamos en la primera parte, la aleatoriedad de las piezas hace que nunca se repitan dos partidas idénticas en toda la historia del universo. Las combinaciones posibles de secuencias y configuraciones del tablero crecen de forma exponencial, alcanzando magnitudes que superan ampliamente las del Go o el Ajedrez.\nEste nivel de entropía plantea un problema fundamental para el aprendizaje por refuerzo: ¿cómo puede el agente aprender estrategias estables y generalizables si cada partida es esencialmente única? Evaluar el valor de una decisión concreta en un contexto tan cambiante requeriría un número astronómico de simulaciones, algo totalmente inviable.\nEsto se debe a que el valor real de una posición en Tetris no depende únicamente del estado actual del tablero, sino también de la enorme combinatoria de piezas que pueden aparecer a continuación. Cada posible secuencia futura altera drásticamente la validez de una jugada presente, haciendo que estimar su impacto sea un problema de profundidad temporal y combinatoria casi infinita.\nPara superar este obstáculo, desarrollé una estrategia diferente: en lugar de intentar batallar contra el azar, decidí domesticarlo. La idea fue crear un segundo agente, un “rival artificial”, cuyo papel no es competir directamente enviando las líneas al jugador —como ocurre en el Tetris multijugador—, sino intervenir en la distribución de las piezas.\nEl funcionamiento es el siguiente: este agente rival observa el tablero en cada momento y selecciona, de entre todas las piezas posibles, aquella que resulte más desfavorable para el jugador principal. Es decir, intenta ofrecerle las piezas que generen las situaciones más complicadas posibles. De este modo, transformamos un entorno originalmente aleatorio en uno determinista y adversarial, en el que cada configuración de tablero tiene su contrapartida más desafiante.\nAmbos agentes —el jugador y su rival— se entrenan de manera paralela y coevolutiva. A medida que el jugador aprende a resolver estructuras cada vez más difíciles, el rival también mejora, descubriendo nuevas formas de romper las estrategias del jugador. Este proceso genera una dinámica de retroalimentación constante.\nEl resultado es un entrenamiento adversarial progresivo, en el que ambos agentes se impulsan mutuamente hacia la mejora continua.\nEstamos creando así un jugador IA capaz de desenvolverse en los peores escenarios posibles.\nY finalmente cuando este agente se enfrenta a una partida real, a veces se encontrará con esas mismas situaciones difíciles que ya ha aprendido a manejar, por lo que sabrá cómo reaccionar y superarlas.\nY otras veces se topará con escenarios más favorables, en los que, gracias a su entrenamiento bajo presión, podrá desenvolverse con aún mayor facilidad.\nResultados Como puede verse en el video, al combinar todos los conceptos descritos anteriormente se ha logrado entrenar un agente de Tetris con un nivel claramente profesional.\nSi no estás familiarizado con las partidas de Tetris competitivo, su estilo de juego puede parecerte extraño: el objetivo tanto de los jugadores expertos como del agente en esta fase no es simplemente eliminar líneas, sino construir estructuras altas y específicas que permitan ejecutar rotaciones avanzadas (conocidas como T-Spins y otras variantes). Estas maniobras son más complejas, pero otorgan una puntuación muy superior a las eliminaciones de líneas convencionales.\nEn este punto ya puedo afirmar que se ha alcanzado un hito en el aprendizaje por refuerzo: el agente ha aprendido, sin supervisión ni ejemplos humanos, a jugar de manera estratégica y previsora en un entorno tan impredecible como Tetris. Este resultado demuestra la capacidad de la IA para descubrir por sí misma conceptos tácticos y patrones de planificación que tradicionalmente se atribuían al razonamiento humano.\nSin embargo, incluso con un nivel tan alto, el objetivo sigue siendo ir más allá. La estrategia de entrenamiento adversarial con el jugador rival ha demostrado ser muy eficaz, pero llega un momento en que la mejora se vuelve extremadamente lenta, ya sea por las limitaciones de hardware —en mi caso, una GPU NVIDIA 3090 de 24 GB— o por el enorme tiempo de entrenamiento necesario para seguir progresando.\nEn las siguientes partes de esta serie mostraré cómo evolucioné desde este punto, introduciendo nuevas técnicas y optimizaciones que permitieron alcanzar un nivel de juego prácticamente inalcanzable.\n","wordCount":"3085","inLanguage":"es","image":"https://joan-projects.github.io/img/Tetris2.png","datePublished":"2025-11-05T00:00:00Z","dateModified":"2025-11-05T00:00:00Z","author":{"@type":"Person","name":"Joan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://joan-projects.github.io/posts/tetris2/"},"publisher":{"@type":"Organization","name":"Joan — Projects —","logo":{"@type":"ImageObject","url":"https://joan-projects.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://joan-projects.github.io/ accesskey=h title="Joan — Projects — (Alt + H)">Joan — Projects —</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://joan-projects.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://joan-projects.github.io/ title=Inicio><span>Inicio</span></a></li><li><a href=https://joan-projects.github.io/posts/ title=Artículos><span>Artículos</span></a></li><li><a href=https://joan-projects.github.io/contacto/ title=Contacto><span>Contacto</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Aprendizaje por refuerzo en Tetris – Parte 2</h1><div class=post-meta><span title='2025-11-05 00:00:00 +0000 UTC'>5 de noviembre de 2025</span>&nbsp;·&nbsp;<span>15 min</span>&nbsp;·&nbsp;<span>Joan</span>&nbsp;|&nbsp;<span>Traducciones:</span><ul class=i18n_list><li><a href=https://joan-projects.github.io/en/posts/tetris2/>En</a></li></ul></div></header><figure class="post-cover vc-single" data-vc-single data-controls=true data-autoplay=false data-loop=false data-muted=false style="position:relative;margin:0 0 1rem"><video class=vc-video preload=metadata playsinline poster=https://joan-projects.github.io/img/Tetris2.png style=width:100%;height:auto;display:block;border-radius:12px;object-fit:cover><source src=https://joan-projects.github.io/video/Tetris2.mp4 type=video/mp4>Tu navegador no soporta el elemento <code>video</code>.
</video>
<button type=button class=vc-play aria-label="Reproducir vídeo" style="position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);font-size:3rem;line-height:1;opacity:.9;background:rgba(0,0,0,.35);color:#fff;padding:.25em .5em;border-radius:.5em;border:0;cursor:pointer;z-index:3">
▶</button></figure><script>(function(){const e=document.currentScript.previousElementSibling;if(!e||!e.matches(".vc-single"))return;const t=e.querySelector("video"),n=e.querySelector(".vc-play"),i=e.dataset.controls==="true",a=e.dataset.autoplay==="true",r=e.dataset.loop==="true",c=e.dataset.muted==="true";function s(){n&&(n.style.display="none")}function o(){n&&(n.style.display="")}i&&(t.controls=!0),r&&(t.loop=!0),c&&(t.muted=!0),a&&s(),n&&n.addEventListener("click",function(e){e.preventDefault();const n=t.play();n&&typeof n.catch=="function"&&n.catch(()=>{}),s()}),t.addEventListener("playing",s),t.addEventListener("pause",o),t.addEventListener("ended",o)})()</script><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Tabla de Contenidos</span></summary><div class=inner><ul><li><a href=#introducci%c3%b3n aria-label=Introducción>Introducción</a></li><li><a href=#primeros-pasos aria-label="Primeros pasos">Primeros pasos</a></li><li><a href=#modelado-de-la-recompensa aria-label="Modelado de la recompensa">Modelado de la recompensa</a></li><li><a href=#incentivar-la-curiosidad aria-label="Incentivar la curiosidad">Incentivar la curiosidad</a></li><li><a href=#agentes-en-jerarqu%c3%ada aria-label="Agentes en jerarquía">Agentes en jerarquía</a></li><li><a href=#aumentaci%c3%b3n-de-datos-resuelta-mediante-atenci%c3%b3n aria-label="Aumentación de datos resuelta mediante atención">Aumentación de datos resuelta mediante atención</a></li><li><a href=#luchando-contra-un-entorno-aleatorio aria-label="Luchando contra un entorno aleatorio">Luchando contra un entorno aleatorio</a></li><li><a href=#resultados aria-label=Resultados>Resultados</a></li></ul></div></details></div><div class=post-content><h2 id=introducción>Introducción<a hidden class=anchor aria-hidden=true href=#introducción>#</a></h2><p>En esta segunda entrega te voy a contar cómo abordé el desafío del entrenamiento de un agente de Reinforcement Learning aplicado a Tetris, teniendo en cuenta las dificultades que mencioné en el artículo anterior.<br>El objetivo principal es alcanzar un nivel sobrehumano, pero no en el sentido de que el agente juegue más rápido o con mayor precisión que una persona —eso, al fin y al cabo, es lo esperable de una máquina—, sino porque sea mejor estratega. De hecho, nuestro agente jugará a una velocidad similar a la de un jugador bueno (menos de 2.5 piezas por segundo), e incluso le obligaré a cometer errores de vez en cuando (5% de probabilidad de cometer error).<br>Voy a contarte las primeras aproximaciones que dieron buenos resultados y que sirvieron de base para las siguientes, que omitiré aquí para no hacer el artículo demasiado extenso ni excesivamente técnico o matemático.<br>Imagino que lo primero que habrás hecho ha sido darle al play y comprobado el sorprendente nivel alcanzado. Es sorprendente y todo un logro para el aprendizaje por refuerzo. He conseguido que una IA aprenda estrategias efectivas por sí sola, sin recibir ningún tipo de consejo; como ves, planifica, es capaz de prever situaciones futuras sin conocer las próximas piezas, realiza combos y no se deja llevar por la recompensa inmediata.<br>Así que, si el video ya te dejó con curiosidad, te invito a seguir leyendo. ¡Vamos allá!</p><h2 id=primeros-pasos>Primeros pasos<a hidden class=anchor aria-hidden=true href=#primeros-pasos>#</a></h2><p>El primer paso —que quizá des por hecho, pero vale la pena mencionar— consiste en implementar un entorno de juego que replique con precisión el comportamiento de una partida real en tetr.io, ya que ese será el escenario final donde quiero que juegue el agente.<br>Sin embargo, no entrenamos directamente en tetr.io. La razón principal es que nuestro entorno necesita estar totalmente optimizado en términos de tiempo, de modo que se puedan ejecutar partidas a la mayor velocidad posible.<br>Para que te hagas una idea, el entorno que desarrollé es capaz de simular partidas cientos de veces más rápido que lo que podríamos lograr jugando directamente en tetr.io.</p><h2 id=modelado-de-la-recompensa>Modelado de la recompensa<a hidden class=anchor aria-hidden=true href=#modelado-de-la-recompensa>#</a></h2><p>Como mencioné en la primera parte, uno de los principales desafíos del Reinforcement Learning aplicado a Tetris es el diseño de la función de recompensa. Las partidas pueden ser potencialmente infinitas, y para entrenar correctamente al agente es fundamental poder determinar qué partida ha sido mejor jugada que otra.<br>Una primera idea sencilla para resolverlo sería limitar el número de piezas por partida, por ejemplo, a 100. De este modo, podríamos comparar el rendimiento de las partidas según su puntuación final, haciendo que el agente aprenda a maximizar su puntuación dentro de ese margen.</p><p>Aunque esta aproximación inicial es válida, presenta varios problemas:</p><ol><li><p><strong>Crecimiento de la dificultad.</strong><br>En Tetris, obtener puntuaciones más altas se vuelve exponencialmente más difícil a medida que avanza la partida. Sin embargo, este modelo lineal de recompensa no refleja esa complejidad, y queremos que el agente aprenda precisamente a reconocer y adaptarse a ese aumento de dificultad.</p></li><li><p><strong>Valor temporal de las puntuaciones.</strong><br>Puede ocurrir que un agente consiga una cierta puntuación rápidamente y luego se estanque, mientras que otro alcance la misma puntuación, pero solo al final de la partida. Si ambos obtienen la misma recompensa final, el sistema considerará que han rendido igual, cuando en realidad el primero ha demostrado una estrategia más eficiente en algunos puntos. Por tanto, debemos diseñar la recompensa de forma que premie los picos de puntuación o las puntuaciones altas tempranas.</p></li><li><p><strong>Estado final de la partida.</strong><br>Otro caso común es que dos partidas terminen con la misma puntuación, pero una de ellas lo haga al borde del colapso, mientras que la otra mantenga el tablero en una situación estable. En ese caso, el modelo básico consideraría que ambas son igual de buenas, cuando la segunda claramente refleja una gestión más sólida. Una forma sencilla de abordar esto es variar el número de piezas por partida: un agente que termina “al borde de la muerte” tras 100 piezas obtendría una puntuación peor cuando se le exija colocar 150, penalizando indirectamente las estrategias inestables.</p></li></ol><h2 id=incentivar-la-curiosidad>Incentivar la curiosidad<a hidden class=anchor aria-hidden=true href=#incentivar-la-curiosidad>#</a></h2><p>Cuando una persona se enfrenta a una situación completamente nueva —por ejemplo, un juego que nunca ha probado—, lo primero que hace es experimentar: prueba diferentes acciones, observa los resultados y aprende de ellos. Este proceso de ensayo y error es esencial para entender cómo funciona el entorno.<br>Nuestro agente debe hacer exactamente lo mismo. No basta con que repita las acciones que le dieron buenos resultados en el pasado; eso lo llevaría rápidamente a un aprendizaje limitado o sesgado. En su lugar, debe mantener una curiosidad activa, explorando también aquellas acciones cuyo efecto desconoce.</p><p>Para conseguir esto, implementé un mecanismo de curiosidad artificial mediante una segunda red neuronal auxiliar, que se ejecuta en paralelo al agente principal. Esta red tiene un objetivo muy concreto: predecir lo que va a ocurrir en el juego cuando el agente realiza una determinada acción.<br>Por ejemplo, si el agente decide colocar una pieza en una posición poco habitual, la red neuronal intenta anticipar el resultado: cómo quedará el tablero, cuántas líneas se eliminarán, si se generarán huecos, etc.<br>Después de que la acción se ejecute realmente, comparamos la predicción de la red con el resultado real.</p><ul><li>Si la predicción ha sido muy precisa, significa que esa acción ya es bien conocida por el agente, y por tanto no se necesita incentivar más su exploración.</li><li>Pero si la predicción falla o difiere mucho de lo ocurrido en la práctica, interpretamos que el agente ha descubierto una situación nueva o poco explorada. En ese caso, se le otorga una recompensa adicional, para motivarlo a seguir investigando ese tipo de acciones.</li></ul><p>Este mecanismo actúa, en esencia, como una fuente de motivación interna: el agente siente “curiosidad” cuando algo le sorprende, y se siente atraído a seguir probando hasta que logra comprenderlo. Una vez domina esa situación, la curiosidad desaparece y su comportamiento vuelve a centrarse en maximizar la recompensa externa (la puntuación del juego).</p><p>Sin embargo, hay un aspecto crucial que conviene destacar.<br>Si la red de predicción mejora demasiado y llega a anticipar con precisión todo lo que puede suceder, el agente deja de experimentar sorpresa ante el entorno. En términos prácticos, esto significa que pierde la curiosidad, ya que todas las acciones le resultan previsibles. Como consecuencia, su capacidad para seguir descubriendo estrategias nuevas o más efectivas se reduce drásticamente.<br>Para evitar este estancamiento, es necesario introducir cierto “olvido controlado” en la red que genera la curiosidad. Dicho de otro modo, hacemos que la red pierda parte de su conocimiento de vez en cuando, o degradamos ligeramente su capacidad de predicción con el tiempo. Esto puede parecer contraproducente —¿por qué querríamos que una red “olvide”?—, pero tiene un propósito muy importante: reactivar la curiosidad del agente hacia situaciones que ya había explorado antes.</p><p>Imagina lo siguiente: al inicio del entrenamiento, el agente todavía no es muy hábil. Cuando se enfrenta a una determinada configuración del tablero y experimenta con una acción nueva, puede que la explore y satisfaga su curiosidad en ese momento, pero sin aprovechar realmente todo su potencial estratégico, porque aún no tiene las habilidades suficientes para hacerlo.<br>Con el tiempo, el agente mejora, desarrolla estrategias más avanzadas y aprende a interpretar el entorno de manera más sofisticada. En ese punto, esas mismas situaciones que antes ya conocía pueden tener nuevas posibilidades que antes no era capaz de reconocer.<br>Si la red de curiosidad conserva intacto su conocimiento anterior, no volverá a considerar esas situaciones como “interesantes”, y el agente nunca regresará a ellas para redescubrirlas con su nueva perspectiva. Por eso es importante que la red “olvide” parcialmente lo aprendido: al hacerlo, vuelve a generar curiosidad ante esos patrones antiguos, permitiendo que el agente los reexplore con un nivel de competencia superior.</p><h2 id=agentes-en-jerarquía>Agentes en jerarquía<a hidden class=anchor aria-hidden=true href=#agentes-en-jerarquía>#</a></h2><p>En muchos videojuegos, las habilidades de un jugador pueden analizarse en dos niveles complementarios: el macrogame y el microgame.<br>El macrogame hace referencia a la dimensión estratégica del juego: la capacidad de planificar, anticiparse a los eventos y tomar decisiones que determinen el rumbo general de la partida. Es la parte que responde al “qué hacer” y “por qué hacerlo”.<br>Por su parte, el microgame describe la dimensión técnica o de ejecución: la precisión, la rapidez y la coordinación necesarias para llevar a cabo de forma efectiva las decisiones estratégicas previamente tomadas. Es la parte que responde al “cómo hacerlo”.</p><p>Si trasladamos estos conceptos al contexto de Tetris, el macrogame corresponde a la planificación global: decidir dónde colocar cada pieza considerando el estado del tablero y, cuando es posible, anticipando las piezas futuras. En cambio, el microgame se refleja en la ejecución exacta de esa decisión: la rapidez con la que se rota la pieza, la secuencia de teclas necesarias y la precisión con que finalmente se coloca en su posición ideal.<br>Ambos niveles son complementarios: el jugador más competente es aquel que logra combinar una visión estratégica sólida con una ejecución precisa y eficiente. Sin embargo, son dos habilidades muy diferentes entre sí, tanto en la forma en que se adquieren como en los procesos cognitivos que las sustentan.</p><p>Con esta distinción en mente, opté por diseñar dos agentes de inteligencia artificial jerarquizados, cada uno especializado en uno de los niveles:</p><ul><li><strong>El agente estratega (macro):</strong> encargado de decidir dónde debe colocarse la siguiente pieza en función del estado del tablero.</li><li><strong>El agente ejecutor (micro):</strong> responsable de traducir esa decisión en acciones concretas, es decir, calcular qué teclas debe presionar y en qué orden para que la pieza llegue a la posición deseada.</li></ul><p>Esta estructura jerárquica presenta múltiples ventajas:</p><ol><li><p><strong>Entrenamiento más eficiente.</strong><br>Cada agente se entrena para cumplir una tarea muy específica, lo que simplifica el proceso de aprendizaje y acelera la convergencia.</p></li><li><p><strong>Separación de errores.</strong><br>Al dividir las responsabilidades, no castigamos una habilidad por los fallos de la otra. Por ejemplo, si el agente estratega toma la mejor decisión posible pero el ejecutor comete un error al colocar la pieza, no penalizamos al estratega por un fallo que no le corresponde. Esto evita confundir la retroalimentación y permite que cada agente aprenda de forma más estable.</p></li><li><p><strong>Diferentes Arquitecturas neuronales.</strong><br>Una de las principales ventajas de dividir el sistema en dos agentes jerarquizados es que podemos diseñar arquitecturas de red neuronal especializadas para cada tipo de tarea. Aunque ambos agentes trabajan con información espacial —por lo que es natural utilizar redes convolucionales (CNNs) para procesar el estado del tablero—, los objetivos y dinámicas de aprendizaje son tan distintos que no tiene sentido utilizar la misma arquitectura ni los mismos hiperparámetros en ambos casos.<br>El agente estratega, encargado del macrogame, necesita una red con mayor profundidad y capacidad de abstracción, capaz de identificar patrones globales y relaciones de largo alcance dentro del tablero. En su caso, conviene incorporar capas de atención o bloques residuales que faciliten la propagación del gradiente y la comprensión de dependencias espaciales complejas. Además, suele beneficiarse de técnicas de regularización moderadas, como dropout o weight decay suaves, para evitar el sobreajuste pero sin limitar su capacidad de exploración. En cuanto a los hiperparámetros, un tamaño de lote mayor (batch size) y tasas de aprendizaje más pequeñas suelen ayudar a estabilizar el proceso, ya que su entrenamiento es más sensible a variaciones pequeñas en la recompensa.<br>Por otro lado, el agente ejecutor, centrado en el microgame, requiere una arquitectura más ligera y reactiva, optimizada para la rapidez y la precisión en la toma de decisiones. En este caso, se priorizan redes más superficiales, con menos capas convolucionales pero más densamente conectadas, lo que permite respuestas inmediatas ante cambios del entorno. Es habitual aplicar reguladores más estrictos, como batch normalization y dropout más agresivos, para forzar la generalización y evitar que memorice secuencias de movimientos concretas. En cuanto a los hiperparámetros, convienen tasas de aprendizaje más altas y entrenamientos con actualizaciones frecuentes, favoreciendo una adaptación ágil a los patrones del juego.</p></li></ol><h2 id=aumentación-de-datos-resuelta-mediante-atención>Aumentación de datos resuelta mediante atención<a hidden class=anchor aria-hidden=true href=#aumentación-de-datos-resuelta-mediante-atención>#</a></h2><p>Como mencioné en la primera parte, una forma eficaz de ayudar al agente a reconocer patrones de juego consiste en exponerlo a situaciones diferentes que comparten la misma solución estratégica. En Tetris, por ejemplo, dos tableros que parecen distintos pueden requerir exactamente la misma jugada si su estructura es simétrica o equivalente en su parte superior.<br>Una forma sencilla de lograr esto es mediante transformaciones del entorno, como aplicar simetrías horizontales tanto al tablero como a las piezas. Esto duplica, por así decirlo, los ejemplos disponibles, ayudando al agente a generalizar sus estrategias. Sin embargo, descubrí que existía una alternativa aún más poderosa y eficiente: introducir un sistema de atención en el propio tablero.</p><p>Este sistema de atención actúa como un filtro visual que limita la cantidad de información que el agente estratega puede procesar simultáneamente. Concretamente, el agente solo puede centrarse en cuatro líneas del tablero a la vez, que en la mayoría de los casos acaban siendo —por pura optimización del entrenamiento— las cuatro líneas superiores, donde se toman las decisiones más críticas.<br>Esta restricción tiene un efecto muy interesante: el agente aprende a abstraer el contexto y a reconocer patrones equivalentes en diferentes alturas del tablero. Es decir, interioriza que la forma de resolver una situación no depende de su posición absoluta, sino de la configuración local de las piezas. En la práctica, esto funciona como una forma de aumentación de datos implícita, ya que el agente aprende que muchos escenarios son esencialmente el mismo problema trasladado verticalmente.</p><p>Gracias a este enfoque, el proceso de entrenamiento inicial se vuelve mucho más eficiente, porque el agente necesita ver menos ejemplos distintos para comprender principios generales de juego. Dicho de otro modo, aprende a “pensar por analogía”: una situación que antes consideraba única pasa a verse como una variación de algo que ya domina.<br>Sin embargo, este sistema de atención debe evolucionar junto con el agente. En las fases tempranas, limitar el foco a solo cuatro líneas facilita el aprendizaje al reducir la complejidad perceptiva. Pero a medida que el agente se vuelve más competente, esa restricción se convierte en una limitación. Para alcanzar un rendimiento de nivel profesional, el agente necesita ampliar progresivamente su campo de atención hasta abarcar la totalidad del tablero, integrando la información de las líneas inferiores para realizar una planificación completa.</p><h2 id=luchando-contra-un-entorno-aleatorio>Luchando contra un entorno aleatorio<a hidden class=anchor aria-hidden=true href=#luchando-contra-un-entorno-aleatorio>#</a></h2><p>Aprender en un entorno tan caótico como una partida de Tetris es, literalmente, un reto épico. Como mencionamos en la primera parte, la aleatoriedad de las piezas hace que nunca se repitan dos partidas idénticas en toda la historia del universo. Las combinaciones posibles de secuencias y configuraciones del tablero crecen de forma exponencial, alcanzando magnitudes que superan ampliamente las del Go o el Ajedrez.<br>Este nivel de entropía plantea un problema fundamental para el aprendizaje por refuerzo: ¿cómo puede el agente aprender estrategias estables y generalizables si cada partida es esencialmente única? Evaluar el valor de una decisión concreta en un contexto tan cambiante requeriría un número astronómico de simulaciones, algo totalmente inviable.</p><p>Esto se debe a que el valor real de una posición en Tetris no depende únicamente del estado actual del tablero, sino también de la enorme combinatoria de piezas que pueden aparecer a continuación. Cada posible secuencia futura altera drásticamente la validez de una jugada presente, haciendo que estimar su impacto sea un problema de profundidad temporal y combinatoria casi infinita.<br>Para superar este obstáculo, desarrollé una estrategia diferente: en lugar de intentar batallar contra el azar, decidí domesticarlo. La idea fue crear un segundo agente, un “rival artificial”, cuyo papel no es competir directamente enviando las líneas al jugador —como ocurre en el Tetris multijugador—, sino intervenir en la distribución de las piezas.<br>El funcionamiento es el siguiente: este agente rival observa el tablero en cada momento y selecciona, de entre todas las piezas posibles, aquella que resulte más desfavorable para el jugador principal. Es decir, intenta ofrecerle las piezas que generen las situaciones más complicadas posibles. De este modo, transformamos un entorno originalmente aleatorio en uno determinista y adversarial, en el que cada configuración de tablero tiene su contrapartida más desafiante.</p><p>Ambos agentes —el jugador y su rival— se entrenan de manera paralela y coevolutiva. A medida que el jugador aprende a resolver estructuras cada vez más difíciles, el rival también mejora, descubriendo nuevas formas de romper las estrategias del jugador. Este proceso genera una dinámica de retroalimentación constante.<br>El resultado es un entrenamiento adversarial progresivo, en el que ambos agentes se impulsan mutuamente hacia la mejora continua.<br>Estamos creando así un jugador IA capaz de desenvolverse en los peores escenarios posibles.<br>Y finalmente cuando este agente se enfrenta a una partida real, a veces se encontrará con esas mismas situaciones difíciles que ya ha aprendido a manejar, por lo que sabrá cómo reaccionar y superarlas.<br>Y otras veces se topará con escenarios más favorables, en los que, gracias a su entrenamiento bajo presión, podrá desenvolverse con aún mayor facilidad.</p><h2 id=resultados>Resultados<a hidden class=anchor aria-hidden=true href=#resultados>#</a></h2><p>Como puede verse en el video, al combinar todos los conceptos descritos anteriormente se ha logrado entrenar un agente de Tetris con un nivel claramente profesional.<br>Si no estás familiarizado con las partidas de Tetris competitivo, su estilo de juego puede parecerte extraño: el objetivo tanto de los jugadores expertos como del agente en esta fase no es simplemente eliminar líneas, sino construir estructuras altas y específicas que permitan ejecutar rotaciones avanzadas (conocidas como T-Spins y otras variantes). Estas maniobras son más complejas, pero otorgan una puntuación muy superior a las eliminaciones de líneas convencionales.<br>En este punto ya puedo afirmar que se ha alcanzado un hito en el aprendizaje por refuerzo: el agente ha aprendido, sin supervisión ni ejemplos humanos, a jugar de manera estratégica y previsora en un entorno tan impredecible como Tetris. Este resultado demuestra la capacidad de la IA para descubrir por sí misma conceptos tácticos y patrones de planificación que tradicionalmente se atribuían al razonamiento humano.<br>Sin embargo, incluso con un nivel tan alto, el objetivo sigue siendo ir más allá. La estrategia de entrenamiento adversarial con el jugador rival ha demostrado ser muy eficaz, pero llega un momento en que la mejora se vuelve extremadamente lenta, ya sea por las limitaciones de hardware —en mi caso, una GPU NVIDIA 3090 de 24 GB— o por el enorme tiempo de entrenamiento necesario para seguir progresando.<br>En las siguientes partes de esta serie mostraré cómo evolucioné desde este punto, introduciendo nuevas técnicas y optimizaciones que permitieron alcanzar un nivel de juego prácticamente inalcanzable.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://joan-projects.github.io/posts/tetris1/><span class=title>Siguiente »</span><br><span>Aprendizaje automático en Tetris – Parte 1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on x" href="https://x.com/intent/tweet/?text=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202&amp;url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f&amp;title=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202&amp;summary=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202&amp;source=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f&title=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on whatsapp" href="https://api.whatsapp.com/send?text=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202%20-%20https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on telegram" href="https://telegram.me/share/url?text=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202&amp;url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje por refuerzo en Tetris – Parte 2 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Aprendizaje%20por%20refuerzo%20en%20Tetris%20%e2%80%93%20Parte%202&u=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://joan-projects.github.io/>Joan — Projects —</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>window.addEventListener("load",function(){if(window.renderMathInElement){const e=document.querySelector("article, .post-content, main, body");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})}})</script><script>(function(){if(window.__VC_LIST_READY__)return;window.__VC_LIST_READY__=!0;function e(e){document.querySelectorAll(".vc-entry video").forEach(t=>{t!==e&&t.pause()})}document.addEventListener("click",function(t){const s=t.target.closest(".vc-poster");if(!s)return;t.preventDefault();const i=s.dataset.mp4,a=s.dataset.webm,r=s.dataset.poster,c=s.dataset.controls==="true",u=s.dataset.autoplay==="true",l=s.dataset.loop==="true",d=s.dataset.muted==="true",n=document.createElement("video");if(n.preload="metadata",n.playsInline=!0,r&&(n.poster=r),c&&(n.controls=!0),l&&(n.loop=!0),d&&(n.muted=!0),a){const e=document.createElement("source");e.src=a,e.type="video/webm",n.appendChild(e)}if(i){const e=document.createElement("source");e.src=i,e.type="video/mp4",n.appendChild(e)}const h=s.parentElement;s.replaceWith(n),e(n);const o=n.play();o&&typeof o.catch=="function"&&o.catch(()=>{})},!1)})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>