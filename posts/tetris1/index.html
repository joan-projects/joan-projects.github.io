<!doctype html><html lang=es dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Aprendizaje automático en Tetris – Parte 1 | Joan — Projects —</title><meta name=keywords content><meta name=description content="Introducción
Comienza aquí una serie de artículos donde te contaré mi viaje para desarrollar una inteligencia artificial capaz de jugar Tetris al máximo nivel, en tiempo real, dentro del entorno tetr.io — El mismo donde compiten hoy los jugadores profesionales.
A lo largo de esta serie compartiré y describiré los hitos conseguidos hasta la publicación del repositorio final con la IA totalmente entrenada.
Nota aclaratoria:
En este documento utilizo el término &ldquo;aprendizaje automático&rdquo; para facilitar la comprensión del lector no técnico.
Sin embargo, es importante señalar que las técnicas descritas corresponden específicamente a métodos de aprendizaje por refuerzo.
El motivo de esta elección terminológica es que el concepto de aprendizaje automático transmite de manera más intuitiva la idea de que el proceso de entrenamiento se realiza sin intervención humana directa, es decir, sin que se proporcionen instrucciones explícitas ni se guíe la estrategia que el sistema debe aprender.
En otras palabras, aunque el término general sea &ldquo;aprendizaje automático&rdquo;, el enfoque real se basa en agentes que aprenden solos mediante interacción con su entorno, optimizando su comportamiento a través de recompensas o penalizaciones."><meta name=author content="Joan"><link rel=canonical href=https://joan-projects.github.io/posts/tetris1/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://joan-projects.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://joan-projects.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://joan-projects.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://joan-projects.github.io/apple-touch-icon.png><link rel=mask-icon href=https://joan-projects.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=es href=https://joan-projects.github.io/posts/tetris1/><link rel=alternate hreflang=en href=https://joan-projects.github.io/en/posts/tetris1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js crossorigin=anonymous></script><style>.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><style>.vc-entry{position:relative}.vc-poster{position:relative;display:block;width:100%;padding:0;border:0;background:0 0;cursor:pointer}.vc-poster .entry-cover-image{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-entry video{display:block;width:100%;height:auto;border-radius:var(--radius,12px)}.vc-play{position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);font-size:3rem;line-height:1;opacity:.9;background:rgba(0,0,0,.35);color:#fff;padding:.25em .5em;border-radius:.5em}@media(max-width:640px){.vc-play{font-size:2.2rem}}.list .post-entry{display:flex;flex-direction:column}.list .post-entry .entry-header{order:0}.list .post-entry .entry-cover{order:1;margin-top:.5rem}</style><meta property="og:url" content="https://joan-projects.github.io/posts/tetris1/"><meta property="og:site_name" content="Joan — Projects —"><meta property="og:title" content="Aprendizaje automático en Tetris – Parte 1"><meta property="og:description" content="Introducción Comienza aquí una serie de artículos donde te contaré mi viaje para desarrollar una inteligencia artificial capaz de jugar Tetris al máximo nivel, en tiempo real, dentro del entorno tetr.io — El mismo donde compiten hoy los jugadores profesionales.
A lo largo de esta serie compartiré y describiré los hitos conseguidos hasta la publicación del repositorio final con la IA totalmente entrenada.
Nota aclaratoria:
En este documento utilizo el término “aprendizaje automático” para facilitar la comprensión del lector no técnico.
Sin embargo, es importante señalar que las técnicas descritas corresponden específicamente a métodos de aprendizaje por refuerzo.
El motivo de esta elección terminológica es que el concepto de aprendizaje automático transmite de manera más intuitiva la idea de que el proceso de entrenamiento se realiza sin intervención humana directa, es decir, sin que se proporcionen instrucciones explícitas ni se guíe la estrategia que el sistema debe aprender.
En otras palabras, aunque el término general sea “aprendizaje automático”, el enfoque real se basa en agentes que aprenden solos mediante interacción con su entorno, optimizando su comportamiento a través de recompensas o penalizaciones."><meta property="og:locale" content="es-es"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-22T00:00:00+00:00"><meta property="og:image" content="https://joan-projects.github.io/img/Tetris1.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://joan-projects.github.io/img/Tetris1.png"><meta name=twitter:title content="Aprendizaje automático en Tetris – Parte 1"><meta name=twitter:description content="Introducción
Comienza aquí una serie de artículos donde te contaré mi viaje para desarrollar una inteligencia artificial capaz de jugar Tetris al máximo nivel, en tiempo real, dentro del entorno tetr.io — El mismo donde compiten hoy los jugadores profesionales.
A lo largo de esta serie compartiré y describiré los hitos conseguidos hasta la publicación del repositorio final con la IA totalmente entrenada.
Nota aclaratoria:
En este documento utilizo el término &ldquo;aprendizaje automático&rdquo; para facilitar la comprensión del lector no técnico.
Sin embargo, es importante señalar que las técnicas descritas corresponden específicamente a métodos de aprendizaje por refuerzo.
El motivo de esta elección terminológica es que el concepto de aprendizaje automático transmite de manera más intuitiva la idea de que el proceso de entrenamiento se realiza sin intervención humana directa, es decir, sin que se proporcionen instrucciones explícitas ni se guíe la estrategia que el sistema debe aprender.
En otras palabras, aunque el término general sea &ldquo;aprendizaje automático&rdquo;, el enfoque real se basa en agentes que aprenden solos mediante interacción con su entorno, optimizando su comportamiento a través de recompensas o penalizaciones."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://joan-projects.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Aprendizaje automático en Tetris – Parte 1","item":"https://joan-projects.github.io/posts/tetris1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Aprendizaje automático en Tetris – Parte 1","name":"Aprendizaje automático en Tetris – Parte 1","description":"Introducción Comienza aquí una serie de artículos donde te contaré mi viaje para desarrollar una inteligencia artificial capaz de jugar Tetris al máximo nivel, en tiempo real, dentro del entorno tetr.io — El mismo donde compiten hoy los jugadores profesionales.\nA lo largo de esta serie compartiré y describiré los hitos conseguidos hasta la publicación del repositorio final con la IA totalmente entrenada.\nNota aclaratoria:\nEn este documento utilizo el término \u0026ldquo;aprendizaje automático\u0026rdquo; para facilitar la comprensión del lector no técnico.\nSin embargo, es importante señalar que las técnicas descritas corresponden específicamente a métodos de aprendizaje por refuerzo.\nEl motivo de esta elección terminológica es que el concepto de aprendizaje automático transmite de manera más intuitiva la idea de que el proceso de entrenamiento se realiza sin intervención humana directa, es decir, sin que se proporcionen instrucciones explícitas ni se guíe la estrategia que el sistema debe aprender.\nEn otras palabras, aunque el término general sea \u0026ldquo;aprendizaje automático\u0026rdquo;, el enfoque real se basa en agentes que aprenden solos mediante interacción con su entorno, optimizando su comportamiento a través de recompensas o penalizaciones.\n","keywords":[],"articleBody":"Introducción Comienza aquí una serie de artículos donde te contaré mi viaje para desarrollar una inteligencia artificial capaz de jugar Tetris al máximo nivel, en tiempo real, dentro del entorno tetr.io — El mismo donde compiten hoy los jugadores profesionales.\nA lo largo de esta serie compartiré y describiré los hitos conseguidos hasta la publicación del repositorio final con la IA totalmente entrenada.\nNota aclaratoria:\nEn este documento utilizo el término “aprendizaje automático” para facilitar la comprensión del lector no técnico.\nSin embargo, es importante señalar que las técnicas descritas corresponden específicamente a métodos de aprendizaje por refuerzo.\nEl motivo de esta elección terminológica es que el concepto de aprendizaje automático transmite de manera más intuitiva la idea de que el proceso de entrenamiento se realiza sin intervención humana directa, es decir, sin que se proporcionen instrucciones explícitas ni se guíe la estrategia que el sistema debe aprender.\nEn otras palabras, aunque el término general sea “aprendizaje automático”, el enfoque real se basa en agentes que aprenden solos mediante interacción con su entorno, optimizando su comportamiento a través de recompensas o penalizaciones.\nTetris: el juego sencillo que desafía al aprendizaje automático En la historia del aprendizaje automático, hay momentos canónicos que son legendarios: AlphaGo y AlphaZero alcanzaron un rendimiento sobrehumano en Go y Ajedrez, demostrando que una inteligencia artificial puede aprender por sí sola, simplemente jugando contra sí misma, sin ninguna ayuda humana, hasta superar a los mejores jugadores del mundo en juegos donde la estrategia lo es todo.\nPero hay un juego que, aunque parece más simple, es un reto todavía mayor para una IA: Tetris.\nSí, ese juego de piezas que caen sin descanso puede ser un auténtico infierno para los sistemas de aprendizaje automático cuando se intenta alcanzar un nivel comparable —o superior— al de un jugador experimentado, y mucho más aún al de un profesional.\nVamos a ver por qué.\nEl número abrumador de posibilidades Para entender la magnitud del reto, basta comparar la cantidad de situaciones posibles en las que un jugador se puede encontrar en cada juego.\nEn Ajedrez, el número de configuraciones posibles del tablero ronda las 4,82 × 10⁴⁴. En Go, se dispara hasta las 2,08 × 10¹⁷⁰.\nEn comparación, Tetris juega en otra liga.\nUn tablero de Tetris de 20 filas por 10 columnas ya ofrece $2^{20\\times10}$ configuraciones posibles, pero eso es solo el principio.\nSi añadimos el factor de la bolsa aleatoria de piezas, y acotamos a una partida corta de unas 200 piezas (aproximadamente un minuto de juego), el número total de posibilidades escala a lo inconcebible:\n(1,37 × 10³⁷¹) × 2²⁰⁰ ≈ 2,2 × 10⁴³¹\nEn otras palabras, el espacio de estados de Tetris es astronómicamente más grande que el de cualquier otro juego clásico.\nTetris no es determinista: El caos de la aleatoriedad Y aún así, el tamaño del espacio de estados no es el mayor problema.\nLa verdadera dificultad radica en que Tetris no es un juego determinista.\nEn Ajedrez o Go, una jugada concreta siempre produce el mismo resultado. Las reglas son fijas y las consecuencias, previsibles.\nEn Tetris, en cambio, una decisión puede ser excelente o desastrosa dependiendo de qué pieza venga después.\nEl contexto cambia constantemente y el entorno es altamente inestable.\nEsto significa que la evaluación de una posición no puede depender solo del tablero actual: el futuro inmediato —impredecible por naturaleza— cambia el valor de cada acción.\nComo consecuencia, el famoso método MCTS (Monte Carlo Tree Search), que fue la piedra angular de AlphaZero y AlphaGo, resulta ineficaz para Tetris. Su estructura depende de escenarios deterministas y con resultados repetibles, justo lo contrario del caos tetrisiano.\nRecompensa, número de acciones y planificación El contraste se amplía cuando miramos la dinámica de decisiones:\nEn Ajedrez, un jugador toma unas 40 decisiones por partida. En Go, alrededor de 100. En Tetris, en cambio, el número de acciones es potencialmente infinito. Y mientras que en Ajedrez o Go la recompensa final es clara —victoria, empate o derrota—, en Tetris la recompensa puede definirse como una puntuación… pero sin un límite superior conocido.\nEsto complica enormemente la función de recompensa que una IA debe aprender.\nUna IA puede hacer una buena partida, pero ¿cómo sabe si su puntuación es “buena” o “excelente”?\nEsta falta de referencia complica el diseño de la función de recompensa, un elemento crucial en el aprendizaje por refuerzo.\nEl problema de la obtención de datos Para que una inteligencia artificial aprenda a jugar Tetris de verdad, no basta con mostrarle unas cuantas partidas: tiene que jugar miles, millones, incluso cientos de millones de veces.\nSolo así puede empezar a entender los patrones del juego y, poco a poco, descubrir por sí misma las estrategias más efectivas.\nPor suerte, los ordenadores juegan infinitamente más rápido que los humanos. Mientras una persona puede jugar unas pocas partidas en una hora, un agente de IA puede disputar millones de partidas en el mismo tiempo, aprendiendo de cada error y cada acierto.\nLo fundamental en este proceso es registrar lo que ocurre en cada momento: qué situación ve el agente, qué acción toma y qué resultado obtiene.\nA cada uno de estos registros lo llamamos una transición.\nCada transición es, en esencia, una pequeña lección que la IA guarda en su memoria para aprender cómo actuar la próxima vez que se enfrente a algo parecido.\nTetris frente a Ajedrez y Go En Ajedrez, una transición es muy rápida de calcular: en la mayoría de casos solo necesitamos verificar si la jugada es legal o no, por contra, en Tetris, no solo eso, sino que es necesario calcular cómo esa jugada afecta al tablero y construir la nueva bolsa de piezas.\nPara ser más precisos, realizar una transición en Ajedrez puede ser cientos de veces más rápido que hacerlo en Tetris.\nPor tanto, el tiempo total para obtener datos de entrenamiento es mucho mayor.\nEn cambio, si comparamos con Go, el tiempo por transición es similar.\nSin embargo —y aquí viene la trampa— la velocidad de simulación no lo es todo.\nAunque los ordenadores parezcan capaces de jugar a velocidades inimaginables, no son suficientes los recursos de un ordenador doméstico para entrenar una IA compleja de este tipo.\nEl problema no está solo en la potencia de cálculo, sino en el tiempo que lleva obtener datos realmente útiles.\nPor qué el tiempo de obtención de datos importa En juegos como el Ajedrez o el Go, las consecuencias suelen hacerse evidentes bastante rápido, normalmente en la siguiente transición. En cambio, en Tetris, cuando se juega a un nivel avanzado, pueden notarse incluso más de diez transiciones después. Es por eso que una mala jugada puede parecer buena durante varias iteraciones, hasta que sus consecuencias aparecen más tarde.\nEsto obliga al agente a mantener un “sentido del futuro”: una memoria que relacione decisiones pasadas con resultados tardíos. Pero ¿cómo hacerlo en Tetris? No sabemos qué fichas van a llegar, por lo que lo que el jugador ve en un momento dado no es facilmente evaluable.\nY si además queremos que el sistema simule distintas alternativas antes de decidirse —como hace el algoritmo MCTS (Monte Carlo Tree Search)—, el tiempo de cómputo crece exponencialmente.\nPuedes imaginarlo como un jugador de ajedrez que analiza mentalmente tres, cuatro o cinco jugadas por adelantado antes de mover una pieza.\nAhora imagina que debe hacerlo millones de veces por segundo: ahí está la dificultad.\nAumentación de datos: multiplicar la experiencia sin jugar más Una técnica muy poderosa para acelerar el aprendizaje es la aumentación de datos (data augmentation).\nConsiste en crear nuevos ejemplos a partir de los que ya tenemos, modificándolos de forma que sigan siendo válidos, pero aporten variedad al entrenamiento.\nUn ejemplo clásico viene del Ajedrez:\nimagina un final de partida con dos torres intentando dar jaque mate a un rey.\nSi el rey enemigo está en la esquina superior derecha del tablero y logramos el mate en tres movimientos, esa misma secuencia es exactamente equivalente por simetría si el rey está en cualquiera de las otras tres esquinas.\nEs decir, de un solo dato obtenemos cuatro, casi gratis.\nEn Ajedrez, casi todas las posiciones pueden duplicarse (×2), y la mayoría cuadruplicarse (×4). En Go, gracias a sus simetrías y rotaciones, podemos hacer un ×8 de cada dato. En Tetris, sin embargo, la situación es menos favorable: el tablero tiene una única orientación posible (la gravedad actúa siempre hacia abajo), por lo que solo podemos duplicar (×2) los datos disponibles. Esto significa que Tetris parte con un hándicap adicional: no solo necesita más datos, sino que además puede generarlos más lentamente y aprovechar menos la aumentación.\nPor qué la aumentación es esencial La aumentación de datos no solo multiplica la cantidad de información disponible, sino que enseña al sistema a reconocer patrones fundamentales.\nAl ver situaciones equivalentes desde distintas perspectivas, la IA aprende a generalizar mejor, a entender qué características del tablero son realmente importantes y cuáles son circunstanciales.\nEs como enseñarle a un perro a reconocer un ladrón, si solo ha visto uno vestido de negro, pensará que todos los ladrones van de negro.\nPero si ha visto ladrones con chaquetas, gorros o camisetas de colores, aprenderá que lo importante no es el color de la ropa, sino el hecho de que se llevan objetos de la casa del dueño.\nHeurística vs. aprendizaje automatizado: dos caminos distintos Antes de la era del aprendizaje automático, al igual que pasó en Ajedrez, se consiguió en Tetris alcanzaron resultados sorprendentes usando métodos heurísticos.\nLa idea era sencilla: simular todas las posiciones posibles tras colocar una o varias piezas y evaluar cada tablero con una función matemática diseñada por humanos.\nPor ejemplo, una heurística típica penaliza los huecos, premia las líneas completas y busca minimizar la altura media de las columnas.\nEl programa genera cientos o miles de tableros posibles y escoge el movimiento que produzca el valor más alto según esa función.\nEl problema es que una heurística no “piensa” realmente.\nSolo ejecuta una fórmula fija, escrita por un desarrollador.\nToda la “inteligencia” reside en la función de evaluación, no en el sistema.\nEsto significa que el programa nunca aprenderá algo nuevo: si el creador no conoce la mejor estrategia, la IA tampoco la descubrirá.\nEl aprendizaje automatizado, en cambio, representa un salto cualitativo.\nNos permite enfrentarnos a problemas en los que la estrategia óptima es desconocida, porque el propio sistema la descubrirá durante el entrenamiento.\nMediante miles de partidas, ensayo y error, y ajustes de valoración, la IA aprende por sí sola qué patrones conducen al éxito, incluso en entornos tan caóticos e impredecibles como Tetris.\nNo necesita que le indiquemos qué características hacen buena una posición: las termina por deducir.\nEsa capacidad de generar conocimiento sin intervención humana directa es lo que convierte al aprendizaje automático —y especialmente al aprendizaje por refuerzo— en una herramienta revolucionaria.\nAdemás, el enfoque heurístico no puede planificar estrategias a varias jugadas de profundidad.\nEsto se debe a que el número de configuraciones crece de forma exponencial con cada pieza añadida (en Tetris, del orden de $162^{n}$).\nSi el algoritmo intentara prever el resultado de, digamos, cinco piezas futuras, debería explorar $162^{5}≈10^{11}$ —una tarea imposible en tiempo real.\nPor tanto, la heurística vive en el presente: solo puede optimizar el movimiento inmediato.\nEl aprendizaje automatizado, en cambio, puede anticipar el futuro al incorporar la recompensa esperada a largo plazo en su proceso de entrenamiento.\nEsto le permite desarrollar estrategias sostenibles, que no solo buscan la ganancia inmediata (como limpiar una línea), sino mantener el tablero estable y adaptable para las próximas piezas.\nCierre Ahora que hemos establecido los principales retos que enfrenta un agente de aprendizaje automático en Tetris, en los próximos capítulos exploraremos paso a paso cómo evolucionó mi sistema hasta llegar al ansiado objetivo:\nuna IA capaz de jugar Tetris al nivel más alto posible.\n","wordCount":"1971","inLanguage":"es","image":"https://joan-projects.github.io/img/Tetris1.png","datePublished":"2025-10-22T00:00:00Z","dateModified":"2025-10-22T00:00:00Z","author":{"@type":"Person","name":"Joan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://joan-projects.github.io/posts/tetris1/"},"publisher":{"@type":"Organization","name":"Joan — Projects —","logo":{"@type":"ImageObject","url":"https://joan-projects.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://joan-projects.github.io/ accesskey=h title="Joan — Projects — (Alt + H)">Joan — Projects —</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://joan-projects.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://joan-projects.github.io/ title=Inicio><span>Inicio</span></a></li><li><a href=https://joan-projects.github.io/posts/ title=Artículos><span>Artículos</span></a></li><li><a href=https://joan-projects.github.io/contacto/ title=Contacto><span>Contacto</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Aprendizaje automático en Tetris – Parte 1</h1><div class=post-meta><span title='2025-10-22 00:00:00 +0000 UTC'>22 de octubre de 2025</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>Joan</span>&nbsp;|&nbsp;<span>Traducciones:</span><ul class=i18n_list><li><a href=https://joan-projects.github.io/en/posts/tetris1/>En</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://joan-projects.github.io/img/Tetris1.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Tabla de Contenidos</span></summary><div class=inner><ul><li><a href=#introducci%c3%b3n aria-label=Introducción>Introducción</a></li><li><a href=#tetris-el-juego-sencillo-que-desaf%c3%ada-al-aprendizaje-autom%c3%a1tico aria-label="Tetris: el juego sencillo que desafía al aprendizaje automático">Tetris: el juego sencillo que desafía al aprendizaje automático</a></li><li><a href=#el-n%c3%bamero-abrumador-de-posibilidades aria-label="El número abrumador de posibilidades">El número abrumador de posibilidades</a></li><li><a href=#tetris-no-es-determinista-el-caos-de-la-aleatoriedad aria-label="Tetris no es determinista: El caos de la aleatoriedad">Tetris no es determinista: El caos de la aleatoriedad</a></li><li><a href=#recompensa-n%c3%bamero-de-acciones-y-planificaci%c3%b3n aria-label="Recompensa, número de acciones y planificación">Recompensa, número de acciones y planificación</a></li><li><a href=#el-problema-de-la-obtenci%c3%b3n-de-datos aria-label="El problema de la obtención de datos">El problema de la obtención de datos</a></li><li><a href=#tetris-frente-a-ajedrez-y-go aria-label="Tetris frente a Ajedrez y Go">Tetris frente a Ajedrez y Go</a></li><li><a href=#por-qu%c3%a9-el-tiempo-de-obtenci%c3%b3n-de-datos-importa aria-label="Por qué el tiempo de obtención de datos importa">Por qué el tiempo de obtención de datos importa</a></li><li><a href=#aumentaci%c3%b3n-de-datos-multiplicar-la-experiencia-sin-jugar-m%c3%a1s aria-label="Aumentación de datos: multiplicar la experiencia sin jugar más">Aumentación de datos: multiplicar la experiencia sin jugar más</a></li><li><a href=#por-qu%c3%a9-la-aumentaci%c3%b3n-es-esencial aria-label="Por qué la aumentación es esencial">Por qué la aumentación es esencial</a></li><li><a href=#heur%c3%adstica-vs-aprendizaje-automatizado-dos-caminos-distintos aria-label="Heurística vs. aprendizaje automatizado: dos caminos distintos">Heurística vs. aprendizaje automatizado: dos caminos distintos</a></li><li><a href=#cierre aria-label=Cierre>Cierre</a></li></ul></div></details></div><div class=post-content><h2 id=introducción>Introducción<a hidden class=anchor aria-hidden=true href=#introducción>#</a></h2><p>Comienza aquí una serie de artículos donde te contaré mi viaje para desarrollar una inteligencia artificial capaz de jugar Tetris al máximo nivel, en tiempo real, dentro del entorno tetr.io — El mismo donde compiten hoy los jugadores profesionales.<br>A lo largo de esta serie compartiré y describiré los hitos conseguidos hasta la publicación del repositorio final con la IA totalmente entrenada.</p><p><strong>Nota aclaratoria:</strong><br>En este documento utilizo el término <strong>&ldquo;aprendizaje automático&rdquo;</strong> para facilitar la comprensión del lector no técnico.<br>Sin embargo, es importante señalar que las técnicas descritas corresponden específicamente a <strong>métodos de aprendizaje por refuerzo</strong>.<br>El motivo de esta elección terminológica es que el concepto de <em>aprendizaje automático</em> transmite de manera más intuitiva la idea de que el proceso de entrenamiento se realiza <strong>sin intervención humana directa</strong>, es decir, sin que se proporcionen instrucciones explícitas ni se guíe la estrategia que el sistema debe aprender.<br>En otras palabras, aunque el término general sea &ldquo;aprendizaje automático&rdquo;, el enfoque real se basa en <strong>agentes que aprenden solos mediante interacción con su entorno</strong>, optimizando su comportamiento a través de recompensas o penalizaciones.</p><h2 id=tetris-el-juego-sencillo-que-desafía-al-aprendizaje-automático>Tetris: el juego sencillo que desafía al aprendizaje automático<a hidden class=anchor aria-hidden=true href=#tetris-el-juego-sencillo-que-desafía-al-aprendizaje-automático>#</a></h2><p>En la historia del aprendizaje automático, hay momentos canónicos que son legendarios: AlphaGo y AlphaZero alcanzaron un rendimiento sobrehumano en Go y Ajedrez, demostrando que una inteligencia artificial puede aprender por sí sola, simplemente jugando contra sí misma, sin ninguna ayuda humana, hasta superar a los mejores jugadores del mundo en juegos donde la estrategia lo es todo.<br>Pero hay un juego que, aunque parece más simple, es un reto todavía mayor para una IA: Tetris.<br>Sí, ese juego de piezas que caen sin descanso puede ser un auténtico infierno para los sistemas de aprendizaje automático cuando se intenta alcanzar un nivel comparable —o superior— al de un jugador experimentado, y mucho más aún al de un profesional.</p><p>Vamos a ver por qué.</p><h2 id=el-número-abrumador-de-posibilidades>El número abrumador de posibilidades<a hidden class=anchor aria-hidden=true href=#el-número-abrumador-de-posibilidades>#</a></h2><p>Para entender la magnitud del reto, basta comparar la cantidad de situaciones posibles en las que un jugador se puede encontrar en cada juego.<br>En Ajedrez, el número de configuraciones posibles del tablero ronda las 4,82 × 10⁴⁴. En Go, se dispara hasta las 2,08 × 10¹⁷⁰.<br>En comparación, Tetris juega en otra liga.<br>Un tablero de Tetris de 20 filas por 10 columnas ya ofrece $2^{20\times10}$
configuraciones posibles, pero eso es solo el principio.<br>Si añadimos el factor de la bolsa aleatoria de piezas, y acotamos a una partida corta de unas 200 piezas (aproximadamente un minuto de juego), el número total de posibilidades escala a lo inconcebible:</p><p>(1,37 × 10³⁷¹) × 2²⁰⁰ ≈ 2,2 × 10⁴³¹</p><p>En otras palabras, el espacio de estados de Tetris es astronómicamente más grande que el de cualquier otro juego clásico.</p><h2 id=tetris-no-es-determinista-el-caos-de-la-aleatoriedad>Tetris no es determinista: El caos de la aleatoriedad<a hidden class=anchor aria-hidden=true href=#tetris-no-es-determinista-el-caos-de-la-aleatoriedad>#</a></h2><p>Y aún así, el tamaño del espacio de estados no es el mayor problema.<br>La verdadera dificultad radica en que Tetris no es un juego determinista.<br>En Ajedrez o Go, una jugada concreta siempre produce el mismo resultado. Las reglas son fijas y las consecuencias, previsibles.<br>En Tetris, en cambio, una decisión puede ser excelente o desastrosa dependiendo de qué pieza venga después.<br>El contexto cambia constantemente y el entorno es altamente inestable.<br>Esto significa que la evaluación de una posición no puede depender solo del tablero actual: el futuro inmediato —impredecible por naturaleza— cambia el valor de cada acción.<br>Como consecuencia, el famoso método MCTS (Monte Carlo Tree Search), que fue la piedra angular de AlphaZero y AlphaGo, resulta ineficaz para Tetris. Su estructura depende de escenarios deterministas y con resultados repetibles, justo lo contrario del caos tetrisiano.</p><h2 id=recompensa-número-de-acciones-y-planificación>Recompensa, número de acciones y planificación<a hidden class=anchor aria-hidden=true href=#recompensa-número-de-acciones-y-planificación>#</a></h2><p>El contraste se amplía cuando miramos la dinámica de decisiones:</p><ul><li>En Ajedrez, un jugador toma unas 40 decisiones por partida.</li><li>En Go, alrededor de 100.</li><li>En Tetris, en cambio, el número de acciones es potencialmente infinito.</li></ul><p>Y mientras que en Ajedrez o Go la recompensa final es clara —victoria, empate o derrota—, en Tetris la recompensa puede definirse como una puntuación… pero sin un límite superior conocido.<br>Esto complica enormemente la función de recompensa que una IA debe aprender.<br>Una IA puede hacer una buena partida, pero ¿cómo sabe si su puntuación es “buena” o “excelente”?<br>Esta falta de referencia complica el diseño de la función de recompensa, un elemento crucial en el aprendizaje por refuerzo.</p><h2 id=el-problema-de-la-obtención-de-datos>El problema de la obtención de datos<a hidden class=anchor aria-hidden=true href=#el-problema-de-la-obtención-de-datos>#</a></h2><p>Para que una inteligencia artificial aprenda a jugar Tetris de verdad, no basta con mostrarle unas cuantas partidas: tiene que jugar miles, millones, incluso cientos de millones de veces.<br>Solo así puede empezar a entender los patrones del juego y, poco a poco, descubrir por sí misma las estrategias más efectivas.<br>Por suerte, los ordenadores juegan infinitamente más rápido que los humanos. Mientras una persona puede jugar unas pocas partidas en una hora, un agente de IA puede disputar millones de partidas en el mismo tiempo, aprendiendo de cada error y cada acierto.<br>Lo fundamental en este proceso es registrar lo que ocurre en cada momento: qué situación ve el agente, qué acción toma y qué resultado obtiene.<br>A cada uno de estos registros lo llamamos una <strong>transición</strong>.<br>Cada transición es, en esencia, una pequeña lección que la IA guarda en su memoria para aprender cómo actuar la próxima vez que se enfrente a algo parecido.</p><h2 id=tetris-frente-a-ajedrez-y-go>Tetris frente a Ajedrez y Go<a hidden class=anchor aria-hidden=true href=#tetris-frente-a-ajedrez-y-go>#</a></h2><p>En Ajedrez, una transición es muy rápida de calcular: en la mayoría de casos solo necesitamos verificar si la jugada es legal o no, por contra, en Tetris, no solo eso, sino que es necesario calcular cómo esa jugada afecta al tablero y construir la nueva bolsa de piezas.<br>Para ser más precisos, realizar una transición en Ajedrez puede ser cientos de veces más rápido que hacerlo en Tetris.<br>Por tanto, el tiempo total para obtener datos de entrenamiento es mucho mayor.<br>En cambio, si comparamos con Go, el tiempo por transición es similar.<br>Sin embargo —y aquí viene la trampa— la velocidad de simulación no lo es todo.<br>Aunque los ordenadores parezcan capaces de jugar a velocidades inimaginables, no son suficientes los recursos de un ordenador doméstico para entrenar una IA compleja de este tipo.<br>El problema no está solo en la potencia de cálculo, sino en el tiempo que lleva obtener datos realmente útiles.</p><h2 id=por-qué-el-tiempo-de-obtención-de-datos-importa>Por qué el tiempo de obtención de datos importa<a hidden class=anchor aria-hidden=true href=#por-qué-el-tiempo-de-obtención-de-datos-importa>#</a></h2><p>En juegos como el Ajedrez o el Go, las consecuencias suelen hacerse evidentes bastante rápido, normalmente en la siguiente transición. En cambio, en Tetris, cuando se juega a un nivel avanzado, pueden notarse incluso más de diez transiciones después.
Es por eso que una mala jugada puede parecer buena durante varias iteraciones, hasta que sus consecuencias aparecen más tarde.<br>Esto obliga al agente a mantener un “sentido del futuro”: una memoria que relacione decisiones pasadas con resultados tardíos.
Pero ¿cómo hacerlo en Tetris? No sabemos qué fichas van a llegar, por lo que lo que el jugador ve en un momento dado no es facilmente evaluable.<br>Y si además queremos que el sistema simule distintas alternativas antes de decidirse —como hace el algoritmo MCTS (Monte Carlo Tree Search)—, el tiempo de cómputo crece exponencialmente.<br>Puedes imaginarlo como un jugador de ajedrez que analiza mentalmente tres, cuatro o cinco jugadas por adelantado antes de mover una pieza.<br>Ahora imagina que debe hacerlo millones de veces por segundo: ahí está la dificultad.</p><h2 id=aumentación-de-datos-multiplicar-la-experiencia-sin-jugar-más>Aumentación de datos: multiplicar la experiencia sin jugar más<a hidden class=anchor aria-hidden=true href=#aumentación-de-datos-multiplicar-la-experiencia-sin-jugar-más>#</a></h2><p>Una técnica muy poderosa para acelerar el aprendizaje es la <strong>aumentación de datos</strong> (<em>data augmentation</em>).<br>Consiste en crear nuevos ejemplos a partir de los que ya tenemos, modificándolos de forma que sigan siendo válidos, pero aporten variedad al entrenamiento.</p><p>Un ejemplo clásico viene del Ajedrez:<br>imagina un final de partida con dos torres intentando dar jaque mate a un rey.<br>Si el rey enemigo está en la esquina superior derecha del tablero y logramos el mate en tres movimientos, esa misma secuencia es exactamente equivalente por simetría si el rey está en cualquiera de las otras tres esquinas.<br>Es decir, de un solo dato obtenemos cuatro, casi gratis.</p><ul><li>En Ajedrez, casi todas las posiciones pueden duplicarse (×2), y la mayoría cuadruplicarse (×4).</li><li>En Go, gracias a sus simetrías y rotaciones, podemos hacer un ×8 de cada dato.</li><li>En Tetris, sin embargo, la situación es menos favorable: el tablero tiene una única orientación posible (la gravedad actúa siempre hacia abajo), por lo que solo podemos duplicar (×2) los datos disponibles.</li></ul><p>Esto significa que Tetris parte con un hándicap adicional: no solo necesita más datos, sino que además puede generarlos más lentamente y aprovechar menos la aumentación.</p><h2 id=por-qué-la-aumentación-es-esencial>Por qué la aumentación es esencial<a hidden class=anchor aria-hidden=true href=#por-qué-la-aumentación-es-esencial>#</a></h2><p>La aumentación de datos no solo multiplica la cantidad de información disponible, sino que enseña al sistema a reconocer patrones fundamentales.<br>Al ver situaciones equivalentes desde distintas perspectivas, la IA aprende a generalizar mejor, a entender qué características del tablero son realmente importantes y cuáles son circunstanciales.<br>Es como enseñarle a un perro a reconocer un ladrón, si solo ha visto uno vestido de negro, pensará que todos los ladrones van de negro.<br>Pero si ha visto ladrones con chaquetas, gorros o camisetas de colores, aprenderá que lo importante no es el color de la ropa, sino el hecho de que se llevan objetos de la casa del dueño.</p><h2 id=heurística-vs-aprendizaje-automatizado-dos-caminos-distintos>Heurística vs. aprendizaje automatizado: dos caminos distintos<a hidden class=anchor aria-hidden=true href=#heurística-vs-aprendizaje-automatizado-dos-caminos-distintos>#</a></h2><p>Antes de la era del aprendizaje automático, al igual que pasó en Ajedrez, se consiguió en Tetris alcanzaron resultados sorprendentes usando métodos heurísticos.<br>La idea era sencilla: simular todas las posiciones posibles tras colocar una o varias piezas y evaluar cada tablero con una función matemática diseñada por humanos.<br>Por ejemplo, una heurística típica penaliza los huecos, premia las líneas completas y busca minimizar la altura media de las columnas.<br>El programa genera cientos o miles de tableros posibles y escoge el movimiento que produzca el valor más alto según esa función.</p><p>El problema es que una heurística no “piensa” realmente.<br>Solo ejecuta una fórmula fija, escrita por un desarrollador.<br>Toda la “inteligencia” reside en la función de evaluación, no en el sistema.<br>Esto significa que el programa nunca aprenderá algo nuevo: si el creador no conoce la mejor estrategia, la IA tampoco la descubrirá.</p><p>El aprendizaje automatizado, en cambio, representa un salto cualitativo.<br>Nos permite enfrentarnos a problemas en los que la estrategia óptima es desconocida, porque el propio sistema la descubrirá durante el entrenamiento.<br>Mediante miles de partidas, ensayo y error, y ajustes de valoración, la IA aprende por sí sola qué patrones conducen al éxito, incluso en entornos tan caóticos e impredecibles como Tetris.<br>No necesita que le indiquemos qué características hacen buena una posición: las termina por deducir.<br>Esa capacidad de generar conocimiento sin intervención humana directa es lo que convierte al aprendizaje automático —y especialmente al aprendizaje por refuerzo— en una herramienta revolucionaria.</p><p>Además, el enfoque heurístico no puede planificar estrategias a varias jugadas de profundidad.<br>Esto se debe a que el número de configuraciones crece de forma exponencial con cada pieza añadida (en Tetris, del orden de $162^{n}$).<br>Si el algoritmo intentara prever el resultado de, digamos, cinco piezas futuras, debería explorar $162^{5}≈10^{11}$ —una tarea imposible en tiempo real.<br>Por tanto, la heurística vive en el presente: solo puede optimizar el movimiento inmediato.<br>El aprendizaje automatizado, en cambio, puede anticipar el futuro al incorporar la recompensa esperada a largo plazo en su proceso de entrenamiento.<br>Esto le permite desarrollar estrategias sostenibles, que no solo buscan la ganancia inmediata (como limpiar una línea), sino mantener el tablero estable y adaptable para las próximas piezas.</p><h2 id=cierre>Cierre<a hidden class=anchor aria-hidden=true href=#cierre>#</a></h2><p>Ahora que hemos establecido los principales retos que enfrenta un agente de aprendizaje automático en Tetris, en los próximos capítulos exploraremos paso a paso cómo evolucionó mi sistema hasta llegar al ansiado objetivo:<br>una IA capaz de jugar Tetris al nivel más alto posible.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://joan-projects.github.io/posts/tetris2/><span class=title>« Anterior</span><br><span>Aprendizaje por refuerzo en Tetris – Parte 2</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on x" href="https://x.com/intent/tweet/?text=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201&amp;url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f&amp;title=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201&amp;summary=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201&amp;source=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f&title=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on whatsapp" href="https://api.whatsapp.com/send?text=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201%20-%20https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on telegram" href="https://telegram.me/share/url?text=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201&amp;url=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Aprendizaje automático en Tetris – Parte 1 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Aprendizaje%20autom%c3%a1tico%20en%20Tetris%20%e2%80%93%20Parte%201&u=https%3a%2f%2fjoan-projects.github.io%2fposts%2ftetris1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://joan-projects.github.io/>Joan — Projects —</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>window.addEventListener("load",function(){if(window.renderMathInElement){const e=document.querySelector("article, .post-content, main, body");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})}})</script><script>(function(){if(window.__VC_LIST_READY__)return;window.__VC_LIST_READY__=!0;function e(e){document.querySelectorAll(".vc-entry video").forEach(t=>{t!==e&&t.pause()})}document.addEventListener("click",function(t){const s=t.target.closest(".vc-poster");if(!s)return;t.preventDefault();const i=s.dataset.mp4,a=s.dataset.webm,r=s.dataset.poster,c=s.dataset.controls==="true",u=s.dataset.autoplay==="true",l=s.dataset.loop==="true",d=s.dataset.muted==="true",n=document.createElement("video");if(n.preload="metadata",n.playsInline=!0,r&&(n.poster=r),c&&(n.controls=!0),l&&(n.loop=!0),d&&(n.muted=!0),a){const e=document.createElement("source");e.src=a,e.type="video/webm",n.appendChild(e)}if(i){const e=document.createElement("source");e.src=i,e.type="video/mp4",n.appendChild(e)}const h=s.parentElement;s.replaceWith(n),e(n);const o=n.play();o&&typeof o.catch=="function"&&o.catch(()=>{})},!1)})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>